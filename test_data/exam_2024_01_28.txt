exam, question_number, sub_question, question_type, extra_material, question, answer
KLAR 1, 1, "a", "Essay", NULL, "Explain the common objectives shared by Bayesian supervised machine learning methods and non-Bayesian supervised machine learning methods.", "Both Bayesian and non-Bayesian supervised machine learning methods share the common objective of making predictions or classifications based on input data. They aim to learn patterns and relationships from labeled data to generalize well to unseen instances."
KLAR 1, 1, "b", "Essay", NULL, "Contrast the representation of models in Bayesian machine learning with that of non-Bayesian approaches, highlighting key differences in terms of parameters. Use linear regression and Bayesian linear regression as examples", "Bayesian machine learning models represent uncertainty explicitly through probability distributions over parameters. In contrast, non-Bayesian approaches often use point estimates for parameters."
KLAR 1, 1, "c", "Essay", NULL, "Considering the continuous evolution of data in real-world scenarios, explain how Bayesian models can be advantageous in terms of updating and adapting to new information compared to non-Bayesian models.", "Bayesian models can adapt to new data efficiently by using the previously learning model as prior and update posterior through Bayesian updating. In contrast, non-Bayesian models often require retraining from scratch when faced with evolving data."
KLAR 1, 1, "d", "Essay", NULL, "What is the difference between a supervised machine learning problem and an unsupervised machine learning problem? Use maximum 75 words.", "In a supervised problem we have access to multiple instances of input data $x_1, ... x_N$ and corresponding outputs $y_1, ... , y_N$, which we denote as our training data. Our aim is to, based on this data, find a model $y = f(x)$ that relates the input and the output. In unsupervised learning we donâ€™t have any outputs in our training data, we only have access to $x_1, . . . x_N$. Here, we instead want to find meaningful patterns in the data."
KLAR 1, 1, "e", "Essay", NULL, "Consider the measurement model: $y_n = x + \epsilon_n, \quad n=1,...,N$, where all noise terms $\epsilon_n ~ \mathcal{N}(0,1^2)$, what is the mean $\mu_{x|y}$ and variance $\sigma^2_{x|y}$ of the posterior? $p(x|y_1, y_2) = \mathcal{N}(x;\mu_{x|y}, \sigma^2_{x|y}$)", "The likelihood is: \[p(y_1, y_2 \mid x) = \mathcal{N}\left(\begin{bmatrix} y_1 \\ y_2 \end{bmatrix}; \begin{bmatrix} 1 \\ 1 \end{bmatrix} x, \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}\right)\]. Together with the expressions for the prior: $p(x) \sim \mathcal{N}(0, 1^2)$, Corollary 1 gives us the Gaussian posterior: \[p(x \mid y_1, y_2) = \mathcal{N}\left(x; \mu_{x \mid y}, \sigma_{x \mid y}^2\right)\] \text{where}\[\sigma_{x \mid y}^2 = \left( 1^{-1} + \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}^{-1} \begin{bmatrix} 1 \\ 1 \end{bmatrix}\right)^{-1} = \frac{2}{3},\]\[\mu_{x \mid y} = \frac{2}{3} \left( 0 + \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}^{-1} \left(\begin{bmatrix} 1 \\ -2 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)\right) = -\frac{1}{6}\]"
KLAR 1, 3, "a", "Essay", NULL, "Let $X ~ p(x)$ be a random variable following a density $p(x)=\frac{1}{Z}exp(-x^4)$ where Z is an unknown constant. Your task is to compute the variance of X, i.e., computing the expectation $E[(X-E[X])^2]$ by using importance sampling. For simplicity, let us choose the proposal $q(x)=\mathcal{N}(0,\sigma^2)$ to be a Gaussian with zero mean and standard deviation $\sigma$. Run the code to plot $p(x)$ and $q(x)$. Select parameter sigma to get a proper proposal distribution. Argue for your choice of sigma. In particular, argue what problem you run into if you pick a sigma: (i) too small (ii) too large.", "(i) $\sigma$ should not be too small, such that the proposal distribution cannot well cover the support of the target distribution. That would lead to regions of the target distribution not containing any samples. (ii) $\sigma$ should not be too big, since many samples would be far away from the target distribution with a consequence that the corresponding weight $w_i \approx 0$ and hence not contributing to the estimation. $\sigma = 1$ seems to be a reasonable pick since the support of proposal distribution is well covered, but also not too big in comparison to the target such that particles would be waisted." 
1, 3, "b", "Programming", code_2024_01_28.py code_question_3_IS, "Use importance sampling to compute $E[(X-E[X])^2]$ with 10000 samples and provide the answer as well as the code."
1, 3, "c", "Programming", code_2024_01_28.py code_question_3_GS, "A Gibbs sampler is considered. Consider the following two-dimensional Gaussian random variable $\begin{bmatrix}x \\ y\end{bmatrix} ~ \mathcal{N}(\begin{bmatrix}5 \\ -5\end{bmatrix}, \begin{bmatrix}4 & -1\\ -1 & 1\end{bmatrix})$ which has the following conditional distributions $p(x|y)=\mathcal{N}(x;\mu_{x|y},\sigma^2_{x|y})$ where $\mu_{x|y}=-y$, $\sigma^2_{x|y}=3$ and $p(y|x)=\mathcal{N}(y;\mu_{y|x}, \sigma^2_{y|x})$, where $\mu_{y|x}=-0.25x-3.75$, $\sigma^2_{y|x}=0.75$. Implement a Gibbs sampler for $p(x,y)$ by sampling from its conditionals $p(x|y)$ and $p(y|x)$. Draw L=1000 samples and plot the sample trajectories. Provide the code below. Make sure your code is readable and comment any non-trivial steps. Plot the sample trajectories."
1, 4, "a", "Essay", "Denote the number of goals made by a certain team in the match $i$ as $y_i$. We assume $y_i$ to be Poisson distributed: $p(y_i = k | \lambda) = \text{Pois}(k; \lambda) = \frac{1}{k!} \lambda^k e^{-\lambda} \quad \text{for} \quad k = 0, 1, 2, \dots, \quad \text{and} \quad \lambda > 0$ where $\lambda$ is a parameter determining how many goals the team does on average during a match. The team plays $\N$ matches with the number of goals being $y_1, \dots, y_N$ for these $N$ matches. We assume that all $y_1, \dots, y_N$ are conditionally independent given $\lambda$. For the parameter $\lambda$, we choose the so-called Gamma distribution: $p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \quad \text{for} \quad \lambda > 0, \quad \alpha, \beta > 0$In an attempt to improve the model used in the mini-project, we want to exploit the information about the number of goals made in a certain match.$ Here, $\Gamma(\alpha)$ denotes the Gamma function. Describe the dependencies between the random variables \(\lambda, y_1, \dots, y_N\) by drawing a Bayesian network graph and write down the corresponding factorization of the joint distribution \(p(\lambda, y_1, \dots, y_N)\)."
