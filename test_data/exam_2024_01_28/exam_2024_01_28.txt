exam, question_number, sub_question, question_type, extra_material, question, answer
KLAR 1, 1, "a", "Essay", NULL, "Explain the common objectives shared by Bayesian supervised machine learning methods and non-Bayesian supervised machine learning methods.", "Both Bayesian and non-Bayesian supervised machine learning methods share the common objective of making predictions or classifications based on input data. They aim to learn patterns and relationships from labeled data to generalize well to unseen instances."
KLAR 1, 1, "b", "Essay", NULL, "Contrast the representation of models in Bayesian machine learning with that of non-Bayesian approaches, highlighting key differences in terms of parameters. Use linear regression and Bayesian linear regression as examples", "Bayesian machine learning models represent uncertainty explicitly through probability distributions over parameters. In contrast, non-Bayesian approaches often use point estimates for parameters."
KLAR 1, 1, "c", "Essay", NULL, "Considering the continuous evolution of data in real-world scenarios, explain how Bayesian models can be advantageous in terms of updating and adapting to new information compared to non-Bayesian models.", "Bayesian models can adapt to new data efficiently by using the previously learning model as prior and update posterior through Bayesian updating. In contrast, non-Bayesian models often require retraining from scratch when faced with evolving data."
KLAR 1, 1, "d", "Essay", NULL, "What is the difference between a supervised machine learning problem and an unsupervised machine learning problem? Use maximum 75 words.", "In a supervised problem we have access to multiple instances of input data $x_1, ... x_N$ and corresponding outputs $y_1, ... , y_N$, which we denote as our training data. Our aim is to, based on this data, find a model $y = f(x)$ that relates the input and the output. In unsupervised learning we donâ€™t have any outputs in our training data, we only have access to $x_1, . . . x_N$. Here, we instead want to find meaningful patterns in the data."
KLAR 1, 1, "e", "Essay", NULL, "Consider the measurement model: $y_n = x + \epsilon_n, \quad n=1,...,N$, where all noise terms $\epsilon_n ~ \mathcal{N}(0,1^2)$, what is the mean $\mu_{x|y}$ and variance $\sigma^2_{x|y}$ of the posterior? $p(x|y_1, y_2) = \mathcal{N}(x;\mu_{x|y}, \sigma^2_{x|y}$)", "The likelihood is: \[p(y_1, y_2 \mid x) = \mathcal{N}\left(\begin{bmatrix} y_1 \\ y_2 \end{bmatrix}; \begin{bmatrix} 1 \\ 1 \end{bmatrix} x, \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}\right)\]. Together with the expressions for the prior: $p(x) \sim \mathcal{N}(0, 1^2)$, Corollary 1 gives us the Gaussian posterior: \[p(x \mid y_1, y_2) = \mathcal{N}\left(x; \mu_{x \mid y}, \sigma_{x \mid y}^2\right)\] \text{where}\[\sigma_{x \mid y}^2 = \left( 1^{-1} + \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}^{-1} \begin{bmatrix} 1 \\ 1 \end{bmatrix}\right)^{-1} = \frac{2}{3},\]\[\mu_{x \mid y} = \frac{2}{3} \left( 0 + \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}^{-1} \left(\begin{bmatrix} 1 \\ -2 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)\right) = -\frac{1}{6}\]"
KLAR 1, 2, "a", "Multiple Choice", "BN240128.png", "Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp B$ (text A is independent of B), "False: the path A-C-B is not blocked at C"
KLAR 1, 2, "b", "Multiple Choice", "BN240128.png", "Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp E$ (text A is independent of E), "True: the path A-C-E is blocked at C (unobserved head-to-head)"
KLAR 1, 2, "c", "Multiple Choice", "BN240128.png", "Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp B | D$ (text A is independent of B given D), "False: the path A-C-B is still not blocked at C (observing D does not change this)"
KLAR 1, 2, "d", "Multiple Choice", "BN240128.png", "Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp B | C$ (text A is independent of B given C), "True: the path A-C-B is blocked at C (observed head-to-tail)"
KLAR 1, 2, "e", "Multiple Choice", "BN240128.png", "Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp E | B$ (text A is independent of E given B), "False: the path A-C-E is unblocked (unobserved head-to-head but with observed descendants at B)"
KLAR 1, 2, "f", "Multiple Choice", "BN240128.png", "Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp D | E$ (text A is independent of D given E), "True: The path A-C-E-D is blocked both at C (unobserved head-to-head) and E (observed head-to-tail)"
KLAR 1, 2, "g", "Multiple Choice", "BN240128.png", "Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp D | C$ (text A is independent of D given C), "False: The path A-C-E-D is neither blocked at C nor E."
KLAR 1, 2, "h", "Multiple Choice", "BN240128.png", "Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp D | C, E$ (text A is independent of D given C and D), "True: The path A-C-E-D is blocked at E (observed head-to-tail)"
KLAR 1, 2, "i", "Essay", "BN240128.png", "Consider the Bayesian network in the image provided. The graphical representation indicates that a joint distribution can be fatorized p(A,B,C,D,E) into a product of terms. What is the factorization encoded in the Bayesian network. $p(A,B,C,D,E)", "$p(A, B, C, D, E) = p(A)p(B|C)p(C|A, E)p(D)p(E|D)$"
KLAR 1, 2, "i", "Essay", "BN240128.png", "Consider the Bayesian network in the image provided. Consider the factorization encoded in the Bayesian network. $p(A,B,C,D,E). Consider all five random variables A, B, C, D, E to be binary. What is the minimum number of parameters needed to specify the joint distribution $p(A, B, C, D, E)$ given the factorization from the image provided.", "To specify $p(A)$,$$p(A = 1) = \alpha, \quad p(A = 0) = 1 - \alpha $$ we need 1 parameter.To specify $p(B | C)$,$$p(B = 1 | C = 0) = \beta_1, \quad p(B = 0 | C = 0) = 1 - \beta_1$$$$p(B = 1 | C = 1) = \beta_2, \quad p(B = 0 | C = 1) = 1 - \beta_2$$ we need 2 parameters. To specify $p(C | A, E)$,$$p(C = 1 | A = 0, E = 0) = \gamma_1, \quad p(C = 0 | A = 0, E = 0) = 1 - \gamma_1$$$$p(C = 1 | A = 1, E = 0) = \gamma_2, \quad p(C = 0 | A = 1, E = 0) = 1 - \gamma_2$$$$p(C = 1 | A = 0, E = 1) = \gamma_3, \quad p(C = 0 | A = 0, E = 1) = 1 - \gamma_3 $$ $$p(C = 1 | A = 1, E = 1) = \gamma_4, \quad p(C = 0 | A = 1, E = 1) = 1 - \gamma_4$$we need 4 parameters. Similarly for $p(D)$ and $p(E | D)$ we need in total $1 + 2 + 4 + 1 + 2 = 10$ parameters.
KLAR 1, 2, "i", "Essay", "BN240128.png", "Consider the Bayesian network in the image provided. Consider the factorization encoded in the Bayesian network. $p(A,B,C,D,E). Consider all five random variables A, B, C, D, E to be binary. What is the minimum number of parameters needed to specify the joint distribution $p(A, B, C, D, E)$ if no assumption about its factorization is made.", "To specify $p(A, B, C, D, E)$ we need to specify all combinations of the joint $$p(A = 1, B = 1, C = 1, D = 1, E = 1) = p_1,$$$$p(A = 0, B = 1, C = 1, D = 1, E = 1) = p_2,$$$$\cdots,$$$$p(A = 0, B = 0, C = 0, D = 0, E = 1) = p_{31},$$$$p(A = 0, B = 0, C = 0, D = 0, E = 0) = 1 - p_1 - \cdots - p_{31}$$That means we need $2^5 - 1 = 31$ parameters. \textbf{Alternative solution:} You can also consider the full factorization of the joint $p(A, B, C, D, E) = p(A)p(B|A)p(C|A, B)p(D|A, B, C)p(E|A, B, C, D)$ which requires $1+2+4+8+16=31$ parameters by following the same line of arguments in the previous exercise."
KLAR 1, 3, "a", "Programming", "code_2024_01_28_template_is.py", "Let $X ~ p(x)$ be a random variable following a density $p(x)=\frac{1}{Z}exp(-x^4)$ where Z is an unknown constant. Your task is to compute the variance of X, i.e., computing the expectation $E[(X-E[X])^2]$ by using importance sampling. For simplicity, let us choose the proposal $q(x)=\mathcal{N}(0,\sigma^2)$ to be a Gaussian with zero mean and standard deviation $\sigma$. Examine the code provided that plots $p(x)$ and $q(x)$. Select the parameter sigma to get a proper proposal distribution. Argue for your choice of sigma. In particular, argue what problem you run into if you pick a sigma: (i) too small (ii) too large.", "(i) $\sigma$ should not be too small, such that the proposal distribution cannot well cover the support of the target distribution. That would lead to regions of the target distribution not containing any samples. (ii) $\sigma$ should not be too big, since many samples would be far away from the target distribution with a consequence that the corresponding weight $w_i \approx 0$ and hence not contributing to the estimation. $\sigma = 1$ seems to be a reasonable pick since the support of proposal distribution is well covered, but also not too big in comparison to the target such that particles would be waisted." 
KLAR 1, 3, "b", "Programming", "code_2024_01_28_template_is.py", "Implement a python routine that computes $E[(X-E[X])^2]$ with 10000 samples", "code_2024_01_28_is.py"
KLAR 1, 3, "c", "Programming", code_2024_01_28_template_gibbs.py, "A Gibbs sampler is considered. Consider the following two-dimensional Gaussian random variable $\begin{bmatrix}x \\ y\end{bmatrix} ~ \mathcal{N}(\begin{bmatrix}5 \\ -5\end{bmatrix}, \begin{bmatrix}4 & -1\\ -1 & 1\end{bmatrix})$ which has the following conditional distributions $p(x|y)=\mathcal{N}(x;\mu_{x|y},\sigma^2_{x|y})$ where $\mu_{x|y}=-y$, $\sigma^2_{x|y}=3$ and $p(y|x)=\mathcal{N}(y;\mu_{y|x}, \sigma^2_{y|x})$, where $\mu_{y|x}=-0.25x-3.75$, $\sigma^2_{y|x}=0.75$. Implement a Gibbs sampler for $p(x,y)$ by sampling from its conditionals $p(x|y)$ and $p(y|x)$. Draw L=1000 samples and plot the sample trajectories. Provide the code below. Make sure your code is readable and comment any non-trivial steps. Plot the sample trajectories.", 
KLAR 1, 4, "a", "Essay", "Denote the number of goals made by a certain team in the match $i$ as $y_i$. We assume $y_i$ to be Poisson distributed: $p(y_i = k | \lambda) = \text{Pois}(k; \lambda) = \frac{1}{k!} \lambda^k e^{-\lambda} \quad \text{for} \quad k = 0, 1, 2, \dots, \quad \text{and} \quad \lambda > 0$ where $\lambda$ is a parameter determining how many goals the team does on average during a match. The team plays $\N$ matches with the number of goals being $y_1, \dots, y_N$ for these $N$ matches. We assume that all $y_1, \dots, y_N$ are conditionally independent given $\lambda$. For the parameter $\lambda$, we choose the so-called Gamma distribution: $p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \quad \text{for} \quad \lambda > 0, \quad \alpha, \beta > 0$In an attempt to improve the model used in the mini-project, we want to exploit the information about the number of goals made in a certain match.$ Here, $\Gamma(\alpha)$ denotes the Gamma function. Describe the dependencies between the random variables \(\lambda, y_1, \dots, y_N\) by writing down the corresponding factorization of the joint distribution \(p(\lambda, y_1, \dots, y_N)\).", "p(\lambda, y_1, \dots, y_N) = p(y_1 \mid \lambda) \cdot \dots \cdot p(y_N \mid \lambda) p(\lambda)"
KLAR 1, 4, "b", "Essay", "Denote the number of goals made by a certain team in the match $i$ as $y_i$. We assume $y_i$ to be Poisson distributed: $p(y_i = k | \lambda) = \text{Pois}(k; \lambda) = \frac{1}{k!} \lambda^k e^{-\lambda} \quad \text{for} \quad k = 0, 1, 2, \dots, \quad \text{and} \quad \lambda > 0$ where $\lambda$ is a parameter determining how many goals the team does on average during a match. The team plays $\N$ matches with the number of goals being $y_1, \dots, y_N$ for these $N$ matches. We assume that all $y_1, \dots, y_N$ are conditionally independent given $\lambda$. For the parameter $\lambda$, we choose the so-called Gamma distribution: $p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \quad \text{for} \quad \lambda > 0, \quad \alpha, \beta > 0$In an attempt to improve the model used in the mini-project, we want to exploit the information about the number of goals made in a certain match.$ Here, $\Gamma(\alpha)$ denotes the Gamma function. We want to compute the posterior $p(\lambda|y_1,\dots, y_N) based on goals from $N$ matches. i) Write down Bayes' formula for this problem. ii) What are observed and latent variables for this problem? iii) What is the prior and likelihood for this problem?", "\begin{enumerate}\item[i)] \[p(\lambda \mid y_1, \dots, y_N) = \frac{p(y_1, \dots, y_N \mid \lambda) p(\lambda)}{p(y_1, \dots, y_N)}\]\item[ii)] Latent variable: $\lambda$. Observed variables: $y_1, \dots, y_N$\item[iii)] Prior distribution: $p(\lambda)$. Likelihood: $p(y_1, \dots, y_N \mid \lambda)$\end{enumerate}"
KLAR 1, 4, "c", "Essay", "Denote the number of goals made by a certain team in the match $i$ as $y_i$. We assume $y_i$ to be Poisson distributed: $p(y_i = k | \lambda) = \text{Pois}(k; \lambda) = \frac{1}{k!} \lambda^k e^{-\lambda} \quad \text{for} \quad k = 0, 1, 2, \dots, \quad \text{and} \quad \lambda > 0$ where $\lambda$ is a parameter determining how many goals the team does on average during a match. The team plays $\N$ matches with the number of goals being $y_1, \dots, y_N$ for these $N$ matches. We assume that all $y_1, \dots, y_N$ are conditionally independent given $\lambda$. For the parameter $\lambda$, we choose the so-called Gamma distribution: $p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \quad \text{for} \quad \lambda > 0, \quad \alpha, \beta > 0$In an attempt to improve the model used in the mini-project, we want to exploit the information about the number of goals made in a certain match.$ Here, $\Gamma(\alpha)$ denotes the Gamma function. We want to compute the posterior $p(\lambda|y_1,\dots, y_N) based on goals from $N$ matches. Show that the posterior is a Gamma distribution $p(\lambda|y_1\dots, y_N) = \text{Gamma}(\lambda;\alpha^*,\Beta^*)$ and determine its parameters $\alpha^*, \Beta^*$ as a function of $\alpha, \Beta, y_1\dots,y_N$", "\begin{equation}\begin{aligned}p(\lambda | y_1, ..., y_N) &\propto p(y_1, ..., y_N | \lambda)p(\lambda) \\&\propto \lambda^{\prod_{i=1}^Ny_i}e^{-N\lambda}\lambda^{\alpha-1}e^{-\beta\lambda} \\&= \lambda^{\prod_{i=1}y_i+\alpha-1}e^{-(\beta+N)\lambda} \\&= \lambda^{\alpha^*-1}e^{-\beta^*\lambda}\\&\propto \text{Gamma}(\lambda;\alpha^*,\beta^*)\end{aligned}\end{equation}This is proportional to a Gamma distribution with \begin{equation*}    \alpha^* = \alpha + \sum_{i=1}^Ny_i\quad \beta^* = \beta + N\end{equation*}Hence, the Gamma distribution is a conjugate prior to the Poisson distribution where the parameters of the posterior are given above."
KLAR 1, 4, "d", "Essay", "Denote the number of goals made by a certain team in the match $i$ as $y_i$. We assume $y_i$ to be Poisson distributed: $p(y_i = k | \lambda) = \text{Pois}(k; \lambda) = \frac{1}{k!} \lambda^k e^{-\lambda} \quad \text{for} \quad k = 0, 1, 2, \dots, \quad \text{and} \quad \lambda > 0$ where $\lambda$ is a parameter determining how many goals the team does on average during a match. The team plays $\N$ matches with the number of goals being $y_1, \dots, y_N$ for these $N$ matches. We assume that all $y_1, \dots, y_N$ are conditionally independent given $\lambda$. For the parameter $\lambda$, we choose the so-called Gamma distribution: $p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \quad \text{for} \quad \lambda > 0, \quad \alpha, \beta > 0$In an attempt to improve the model used in the mini-project, we want to exploit the information about the number of goals made in a certain match.$ Here, $\Gamma(\alpha)$ denotes the Gamma function. We want to compute the posterior $p(\lambda|y_1,\dots, y_N) based on goals from $N$ matches. Based on the Gamma posterior $p(\lambda|y_1,\dots,y_N) = $ Gamma $(\lambda;\alpha^*,\beta^*)$, you now want to find the predictive distribution $p(y^*|y_1,\dots,y_N)$, i.e., the distribution of the number of goals in a new match $y^*$ conditioned on the number of goals scored in all previous $N$ matches. Show that the predictive distribution is a negative binomial distribution $p(y^*=k|y_1,\dots,y_N) = \text{Neg-bin}(k;r,p) = \frac{\Gamma(k+r)}{k!\Gamma(r)}(1-p)^kp^r$ and determine the parameters $p$ and $r$ above.", "The predictive distribution $p(y^*|y_1,\dots,y_N)$ we get by multiplying the likelihood for the new datapoint $y^*$ with the posterior and marginalize out $\lambda$. \begin{equation}\begin{aligned}p(y^*|y_1,\dots,y_N) &= \int_0^{\infty} p(y^*|\lambda)p(\lambda|y_1,\dots,y_N)d\lambda \\ &= \int_0^{\infty}\frac{\lambda^{y^*}e^{-\lambda}}{y^*!}\frac{(\beta^*)^{\alpha^*}\lambda^{\alpha^*-a}e^{-\beta^*\lambda}}{\Gamma(\alpha^*)} \\&=\frac{(\beta^*)^{\alpha^*}\Gamma(y^*+\alpha^*)}{(\beta^*+1)^{y^*+\alpha^*}y^*!\Gamma(\alpha^*)} \int_0^{\infty} \frac{(\beta^*+1)^{y^*+\alpha^*}\lambda^{y^*+\alpha^*-1}e^{-(\beta^*+1)\lambda}}{\Gamma(y^*+\alpha^*)} d\lambda\\&= \frac{(\beta^*)^{\alpha^*}\Gamma(y^*+\alpha^*)}{(\beta^*+1)^{y^*+\alpha^*}y^*!\Gamma(\alpha^*)} \int_0^{\infty}\text{Gamma}(\lambda;y^*+\alpha^*,\beta^*+1)d\lambda \\&=\frac{(\beta^*)^{\alpha^*}\Gamma(y^*+\alpha^*)}{(\beta^*+1)^{y^*+\alpha^*}y^*!\Gamma(\alpha^*)}\\&=\frac{\Gamma(y^*+\alpha^*}{y^*!\Gamma(\alpha^*)}\left( \frac{\beta^*}{\beta^*+1}\right)^{\alpha^*} \left(1-\frac{\beta^*}{\beta^*+1} \right)^{y^*} \\&= \textbf{Neg-bin}\left(y^*;\alpha^*,\frac{\beta^*}{\beta^*+1} \right)\end{aligned}\end{equation}"
