exam; question_number; sub_question; question_type; extra_material; question; answer
4; 1; "a"; "Multiple choice"; "NULL"; "Select whether the following statement apply to 1. Frequentist, 2. Bayesian viewpoint or 3. both. Can be used to quantify the uncertainty of an estimate."; "Both. Given an estimate, frequentist can quantify the uncertanty using confidence intervals. Bayesian estimate the full probability of an estimate."
4; 1; "b"; "Multiple choice"; "NULL"; "Select whether the following statement apply to 1. Frequentist, 2. Bayesian viewpoint or 3. both. Parameters underlying an experiment are fixed values (deterministic)."; "Frequentist."
4; 1; "c"; "Multiple choice"; "NULL"; "Select whether the following statement apply to 1. Frequentist, 2. Bayesian viewpoint or 3. both. Probabilities are beliefs. Starting with a prior belief the probability can be updated with experimental observations."; "Bayesian"
4; 1; "d"; "Multiple choice"; "NULL"; "Select whether the following statement apply to 1. Frequentist, 2. Bayesian viewpoint or 3. both. Parameters underlying an experiment are random variables."; "Bayesian."
4; 1; "e"; "Multiple choice"; "NULL"; "Consider: $y=0.1x_1+0.2x_2+0.3+\epsilon$, where: $\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \sim  \mathcal{N}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0.8 & 0.2 \\ 0.2 & 0.8 \end{bmatrix}\right)$ and $\epsilon \sim \mathcal{N}(0,0.2)$ is sampled independently from $x_1, x_2$. What is the probability distribution of y: 1. Poisson, 2. None of the above, 3. Weibull, 4. Bernoulli, 5. Gaussian. Select one alternative."; "Gaussian. y has a Gaussian distribution, since it is obtained by the affine transformation of Gaussians (Corollary 2)."
4; 1; "f"; "Multiple choice"; "NULL"; "Consider: $y=0.1x_1+0.2x_2+0.3+\epsilon$, where: $\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \sim  \mathcal{N}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0.8 & 0.2 \\ 0.2 & 0.8 \end{bmatrix}\right)$ and $\epsilon \sim \mathcal{N}(0,0.2)$ is sampled independently from $x_1, x_2$. What is the mean of y? (1) 0.1, (2) 0.3, (3) 0.2, (4) 0.4, (5) 0.8. Select one alternative."; "(2) 0.3 since $E[y] = E[0.1x_1 + 0.2x_2 + 0.3 + \epsilon] = 0.1E[x_1] + 0.2E[x_2] + 0.3 + E[\epsilon] = 0.1\cdot 0 + 0.2\cdot 0 + 0.3 + 0 = 0.3$"
4; 1; "g"; "Multiple choice"; "NULL"; "Consider: $y=0.1x_1+0.2x_2+0.3+\epsilon$, where: $\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \sim  \mathcal{N}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0.8 & 0.2 \\ 0.2 & 0.8 \end{bmatrix}\right)$ and $\epsilon \sim \mathcal{N}(0,0.2)$ is sampled independently from $x_1, x_2$. What is the variance of y? (1) 0.248, (2) 0.3, (3) 0.2, (4) 0.344, (5) 0.4. Select one alternative."; "0.248 since $\text{Var}[y] = \begin{bmatrix} 0.1 & 0.2 & 1 \end{bmatrix}\begin{bmatrix} 0.8 & 0.2 & 0 \\ 0.2 & 0.8 & 0 \\ 0 & 0 & 0.2 \end{bmatrix}  \begin{bmatrix} 0.1 \\ 0.2 \\ 1 \end{bmatrix} = 0.248$"
4; 1; "h"; "Multiple choice"; "NULL"; "Consider: $y=0.1x_1+0.2x_2+0.3+\epsilon$, where: $\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \sim  \mathcal{N}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0.8 & 0.2 \\ 0.2 & 0.8 \end{bmatrix}\right)$ and $\epsilon \sim \mathcal{N}(0,0.2)$ is sampled independently from $x_1, x_2$. If instead we choose $\epsilon$ an uniformly distributed random variable with the same mean and the same variance. 1. y will have the same distribution, but different mean and variance. 2. Nothing will change. 3. y will have a different distribution, but the same mean and variance. 4. y will have a different distribution and variance, but the same mean. 5. y will have a different distribution, variance and mean. Select one alternative."; " y will have a different distribution, but the same mean and variance."
4; 2; "a"; "Essay"; "BNupg2.png"; "Consider the Bayesian network in the image provided. The graphical representation indicates that a joint distribution can be fatorized p(A,B,C,D,E,F,G) into a product of terms. What is the factorization encoded in the Bayesian network?"; "$p(A, B, C, D, E, F ) = p(A)p(B)p(C)p(D)p(E|A, B)p(F |C, D)p(G|E, F )$"
4; 2; "b"; "Multiple choice"; "BNupg2.png"; "Consider the Bayesian network in the image provided. True or false:  $A \perp\!\!\!\perp B$ (text: A is independent of B)"; "True: the path between A and B is blocked. The path A-E-B does not have observed descendants neither is D observed hence it is blocked."
4; 2; "c"; "Multiple choice"; "BNupg2.png"; "Consider the Bayesian network in the image provided. True or false:  $A \perp\!\!\!\perp G$ (text: A is independent of G)"; "False: there is a non-blocked path A-E-G."
4; 2; "d"; "Multiple choice"; "BNupg2.png"; "Consider the Bayesian network in the image provided. True or false: $A \perp\!\!\!\perp B|D$ (text: A is independent of B given D)"; "True: the path between A and B is blocked. The path A-E-B does not have observed descendants neither is D observed hence it is blocked."
4; 2; "e"; "Multiple choice"; "BNupg2.pn"g"; "Consider the Bayesian network in the image provided. True or false: $A \perp\!\!\!\perp B|E$ (text: A is independent of B given E)"; "False: the path A-E-B is unblocked since E is observed."
4; 2; "f"; "Multiple choice"; "BNupg2.png"; "Consider the Bayesian network in the image provided. True or false: $A \perp\!\!\!\perp B|G$ (text: A is independent of B given G)"; "False: the path A-E-B is unblocked since G, which is a descendent of E, is observed."
4; 2; "g"; "Multiple choice"; "BNupg2.pn"g"; "Consider the Bayesian network in the image provided. True or false: $A \perp\!\!\!\perp G|E$ (text: A is independent of G given E)"; "True: the path A-E-G is blocked since E is observed."
4; 2; "h"; "Multiple choice"; "BNupg2.png"; "Consider the Bayesian network in the image provided. True or false: $A \perp\!\!\!\perp C|G$ (text: A is independent of C given G)"; "False: the path A-E-G-F-C is unblocked, since G is observed."
4; 2; "i"; "Multiple choice"; "BNupg2.png"; "Consider the Bayesian network in the image provided. True or false: $A \perp\!\!\!\perp C|G,E$ (text: A is independent of C given G and E)"; "True: the path A-E-G-F-C is blocked, since E is observed"
4; 3; "a"; "Numeric entry"; "NULL"; "Assume that you have two binary random variables X  and Y. And that you know their joint probability distribution: $p(X=0,Y=0)=0.3$, $p(X=0,Y=1)=0.1$, $p(X=1,Y=0)=0.2$, $p(X=1,Y=1)=0.4$. Compute the following expectation analytically: $E{X+Y}$."; "1.1"
4; 3; "b"; "Numeric entry"; "NULL"; "Assume that you have two binary random variables X  and Y. And that you know their joint probability distribution: $p(X=0,Y=0)=0.3$, $p(X=0,Y=1)=0.1$, $p(X=1,Y=0)=0.2$, $p(X=1,Y=1)=0.4$. Compute the following expectation analytically: $E{XY^2}$."; "0.4"
4; 3; "c"; "Numeric entry"; "NULL"; "Assume that you have two binary random variables X  and Y. And that you know their joint probability distribution: $p(X=0,Y=0)=0.3$, $p(X=0,Y=1)=0.1$, $p(X=1,Y=0)=0.2$, $p(X=1,Y=1)=0.4$. Compute the following expectation analytically: $E{cos(\pi X)}$."; "-0.2"
4; 3; "d"; "Numeric entry"; "NULL"; "Assume that you have two binary random variables X  and Y. And that you know their joint probability distribution: $p(X=0,Y=0)=0.3$, $p(X=0,Y=1)=0.1$, $p(X=1,Y=0)=0.2$, $p(X=1,Y=1)=0.4$. Compute the following conditional probabilities: $P(X=1|Y=0)$."; "0.4 since we have the marginal probabilities $$ P(X = 1) = P(X = 1, Y = 0) + P(X = 1, Y = 1) = 0.6 $$ $$ P(Y = 1) = P(X = 0, Y = 1) + P(X = 1, Y = 1) = 0.5 $$ hence: $$ P(X = 1 | Y = 0) = \frac{P(X = 1, Y = 0)}{1 - P(Y = 1)} = \frac{0.2}{0.5} = 0.4 $$"
4; 3; "e"; "Numeric entry"; "NULL"; "Assume that you have two binary random variables X  and Y. And that you know their joint probability distribution: $p(X=0,Y=0)=0.3$, $p(X=0,Y=1)=0.1$, $p(X=1,Y=0)=0.2$, $p(X=1,Y=1)=0.4$. Compute the following conditional probabilities: $P(X=1|Y=1)$."; "0.8 since we have the marginal probabilities $$ P(X = 1) = P(X = 1, Y = 0) + P(X = 1, Y = 1) = 0.6 $$ $$ P(Y = 1) = P(X = 0, Y = 1) + P(X = 1, Y = 1) = 0.5 $$ hence: $$ P(X = 1 | Y = 1) = \frac{P(X = 1, Y = 1)}{P(Y = 1)} = \frac{0.4}{0.5} = 0.8 $$"
4; 3; "f"; "Numeric entry"; "NULL"; "Assume that you have two binary random variables X  and Y. And that you know their joint probability distribution: $p(X=0,Y=0)=0.3$, $p(X=0,Y=1)=0.1$, $p(X=1,Y=0)=0.2$, $p(X=1,Y=1)=0.4$. Compute the following conditional probabilities: $P(Y=1|X=0)$."; "0.25 since we have the marginal probabilities $$ P(X = 1) = P(X = 1, Y = 0) + P(X = 1, Y = 1) = 0.6 $$ $$ P(Y = 1) = P(X = 0, Y = 1) + P(X = 1, Y = 1) = 0.5 $$ hence: $$P(Y = 1 | X = 0) = \frac{P(X = 0, Y = 1)}{1 - P(X = 1)} = \frac{0.1}{0.4} = 0.25 $$"
4; 3; "g"; "Numeric entry"; "NULL"; "Assume that you have two binary random variables X  and Y. And that you know their joint probability distribution: $p(X=0,Y=0)=0.3$, $p(X=0,Y=1)=0.1$, $p(X=1,Y=0)=0.2$, $p(X=1,Y=1)=0.4$. Compute the following conditional probabilities: $P(Y=1|X=1)$."; "0.667 since we have the marginal probabilities $$ P(X = 1) = P(X = 1, Y = 0) + P(X = 1, Y = 1) = 0.6 $$ $$ P(Y = 1) = P(X = 0, Y = 1) + P(X = 1, Y = 1) = 0.5 $$ hence: $$P(Y = 1 | X = 1) = \frac{P(X = 1, Y = 1)}{P(X = 1)} = \frac{0.4}{0.6} = 0.667$$"
4; 3; "h"; "Programming"; "code.py"; "Assume that you have two binary random variables X  and Y. And that you know their joint probability distribution: $p(X=0,Y=0)=0.3$, $p(X=0,Y=1)=0.1$, $p(X=1,Y=0)=0.2$, $p(X=1,Y=1)=0.4$. Implement a code to draw samples $(x_i, y_i),i=1,...L$ using a Gibbs sampler and approximate $E{XY}. Plot the trajectory of the estimation as a function of the number of samples, for L=1 to 10000 samples. What is burn-in period in a Gibbs sampler? Explain what you would need change in the code if you included a burn-in period."; "answer.py"
4; 4; "a"; "Multiple choice"; "NULL"; "Suppose that you are developing an optical instrument for the James Webb Space Telescope. The instrument will be used to study distant stars. Your task is to evaluate and compare different designs (e.g. different number of lenses and corresponding focal lengths). You conclude that it would probably be helpful to simulate what the resulting images would look like (without launching the instrument to space first!). You have access to a relatively large number of images, captured with a range of known designs, and would like to build a simulator that can synthesize new, realistic, images corresponding to a given design. The instrument has an image sensor of size 100 x 100 pixels. Independently of each other, each pixel corresponds to either a star, with probability $p$, or empty space, with probability $1-p$. Star pixels have different intensities distributed according to $I \sim \mathcal{N}(\mu_I,\sigma_I^2) while non-star pixels have zero intensity. Each pixel in the entire final image corresponds to the intensity plus some noise. The noise is Gaussian, with zero mean and variance $\sigma_d^2$ (known from the design). The noise in each pixel is independent and identically distributed. What type of machine learning approach describes this best? 1. Reinforcement learning, 2. Unsupervised learning, 3. Supervised learning, 4. Semi-supervised learning. Select one alternative."; "Since the goal is to generate data similar to a training data set, it is a form of unsupervised learning, more specifically generative modeling."
4; 4; "b"; "Diagram"; "NULL"; "Suppose that you are developing an optical instrument for the James Webb Space Telescope. The instrument will be used to study distant stars. Your task is to evaluate and compare different designs (e.g. different number of lenses and corresponding focal lengths). You conclude that it would probably be helpful to simulate what the resulting images would look like (without launching the instrument to space first!). You have access to a relatively large number of images, captured with a range of known designs, and would like to build a simulator that can synthesize new, realistic, images corresponding to a given design. The instrument has an image sensor of size 100 x 100 pixels. Independently of each other, each pixel corresponds to either a star, with probability $p$, or empty space, with probability $1-p$. Star pixels have different intensities distributed according to $I \sim \mathcal{N}(\mu_I,\sigma_I^2)$ while non-star pixels have zero intensity. Each pixel in the entire final image corresponds to the intensity plus some noise. The noise is Gaussian, with zero mean and variance $\sigma_d^2$ (known from the design). The noise in each pixel is independent and identically distributed. Draw a Bayesian network corresponding to the model."; "BN22_10_21.png"
4; 4; "c"; "Numeric entry"; "NULL"; "Suppose that you are developing an optical instrument for the James Webb Space Telescope. The instrument will be used to study distant stars. Your task is to evaluate and compare different designs (e.g. different number of lenses and corresponding focal lengths). You conclude that it would probably be helpful to simulate what the resulting images would look like (without launching the instrument to space first!). You have access to a relatively large number of images, captured with a range of known designs, and would like to build a simulator that can synthesize new, realistic, images corresponding to a given design. The instrument has an image sensor of size 100 x 100 pixels. Independently of each other, each pixel corresponds to either a star, with probability $p$, or empty space, with probability $1-p$. Star pixels have different intensities distributed according to $I \sim \mathcal{N}(\mu_I,\sigma_I^2) while non-star pixels have zero intensity. Each pixel in the entire final image corresponds to the intensity plus some noise. The noise is Gaussian, with zero mean and variance $\sigma_d^2$ (known from the design). The noise in each pixel is independent and identically distributed. Suppose that you use this model to simulate 3 images $\mathbf{X}$ of the same field of view (the same piece of sky) using $\mu_I = 3$, $\sigma_I^2 = 4$, and $\sigma_d^2 = 3$. In the pixel corresponding to star $k$ you record the following independent observations: $\mathbf{X}_k = \{1, 4, 1\}$. What is the mean of the posterior distribution $p(I_k | \mathbf{X}_k)$ of that star's intensity?"; "$\mu_{I_k | \mathbf{X}_k} = 2.2$ since: The prior is $$p(I_k) = \mathcal{N}(I_k: \mu_I, \sigma_I^2), $$ and the likelihood is $$ p(\mathbf{X}_k | I_k) = \prod_{i=1}^{3} \mathcal{N}(X_{ki}; I_k, \sigma_d^2) = \mathcal{N} \left( \mathbf{X}_k; \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} I_k, \begin{pmatrix} \sigma_d^2 & 0 & 0 \\ 0 & \sigma_d^2 & 0 \\ 0 & 0 & \sigma_d^2 \end{pmatrix} \right).$$ Hence, the model is a Gauss-Gauss conjugate pair, and the posterior is the Gaussian distribution given by Corollary 1 on the Formula Sheet using $$ x_a = I_k, \quad x_b = \mathbf{X}_k = (1, 4, 1)^\top, $$ $$ \mu_a = \mu_I = 3, \quad \Sigma_a = \sigma_I^2 = 4, $$ $$ A = (1, 1, 1)^\top, \quad b = 0, $$ $$ \Sigma_{b|a} = \text{diag}(\sigma_d^2) = I_{3 \times 3}, $$ namely, $$ p \left( I_k | \mathbf{X}_k = (1, 4, 1)^\top \right) = \mathcal{N}(I_k; \mu_{I_k | \mathbf{X}_k}, \Sigma_{I_k | \mathbf{X}_k}), $$ where $\mu_{I_k | \mathbf{X}_k} = 2.2$ and $\Sigma_{I_k | \mathbf{X}_k} = 0.8$."
4; 4; "c"; "Numeric entry"; "NULL"; "Suppose that you are developing an optical instrument for the James Webb Space Telescope. The instrument will be used to study distant stars. Your task is to evaluate and compare different designs (e.g. different number of lenses and corresponding focal lengths). You conclude that it would probably be helpful to simulate what the resulting images would look like (without launching the instrument to space first!). You have access to a relatively large number of images, captured with a range of known designs, and would like to build a simulator that can synthesize new, realistic, images corresponding to a given design. The instrument has an image sensor of size 100 x 100 pixels. Independently of each other, each pixel corresponds to either a star, with probability $p$, or empty space, with probability $1-p$. Star pixels have different intensities distributed according to $I \sim \mathcal{N}(\mu_I,\sigma_I^2) while non-star pixels have zero intensity. Each pixel in the entire final image corresponds to the intensity plus some noise. The noise is Gaussian, with zero mean and variance $\sigma_d^2$ (known from the design). The noise in each pixel is independent and identically distributed. Suppose that you use this model to simulate 3 images $\mathbf{X}$ of the same field of view (the same piece of sky) using $\mu_I = 3$, $\sigma_I^2 = 4$, and $\sigma_d^2 = 3$. In the pixel corresponding to star $k$ you record the following independent observations: $\mathbf{X}_k = \{1, 4, 1\}$. What is the covariance of the posterior distribution $p(I_k | \mathbf{X}_k)$ of that star's intensity?" $\Sigma_{I_k | \mathbf{X}_k} = 0.8$ since: The prior is $$p(I_k) = \mathcal{N}(I_k; \mu_I, \sigma_I^2), $$ and the likelihood is $$ p(\mathbf{X}_k | I_k) = \prod_{i=1}^{3} \mathcal{N}(X_{ki}; I_k, \sigma_d^2) = \mathcal{N} \left( \mathbf{X}_k; \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} I_k, \begin{pmatrix} \sigma_d^2 & 0 & 0 \\ 0 & \sigma_d^2 & 0 \\ 0 & 0 & \sigma_d^2 \end{pmatrix} \right).$$ Hence, the model is a Gauss-Gauss conjugate pair, and the posterior is the Gaussian distribution given by Corollary 1 on the Formula Sheet using $$ x_a = I_k, \quad x_b = \mathbf{X}_k = (1, 4, 1)^\top, $$ $$ \mu_a = \mu_I = 3, \quad \Sigma_a = \sigma_I^2 = 4, $$ $$ A = (1, 1, 1)^\top, \quad b = 0, $$ $$ \Sigma_{b|a} = \text{diag}(\sigma_d^2) = I_{3 \times 3}, $$ namely, $$ p \left( I_k | \mathbf{X}_k = (1, 4, 1)^\top \right) = \mathcal{N}(I_k; \mu_{I_k | \mathbf{X}_k}, \Sigma_{I_k | \mathbf{X}_k}), $$ where $\mu_{I_k | \mathbf{X}_k} = 2.2$ and $\Sigma_{I_k | \mathbf{X}_k} = 0.8$." 
4; 4; "d"; "Essay"; "BN22_10_21.png"; "Suppose that you are developing an optical instrument for the James Webb Space Telescope. The instrument will be used to study distant stars. Your task is to evaluate and compare different designs (e.g. different number of lenses and corresponding focal lengths). You conclude that it would probably be helpful to simulate what the resulting images would look like (without launching the instrument to space first!). You have access to a relatively large number of images, captured with a range of known designs, and would like to build a simulator that can synthesize new, realistic, images corresponding to a given design. The instrument has an image sensor of size 100 x 100 pixels. Independently of each other, each pixel corresponds to either a star, with probability $p$, or empty space, with probability $1-p$. Star pixels have different intensities distributed according to $I \sim \mathcal{N}(\mu_I,\sigma_I^2) while non-star pixels have zero intensity. Each pixel in the entire final image corresponds to the intensity plus some noise. The noise is Gaussian, with zero mean and variance $\sigma_d^2$ (known from the design). The noise in each pixel is independent and identically distributed. The image provided corresponds to the BN of this model. Explain how simulation of a new image can be achieved by forward sampling according to the Bayesian network from the image. Use maximum 100 words"; "Each pixel can be generated independently (sequentially or in parallel) as follows. First, determine whether the pixel corresponds to a star or not by sampling $s \sim \text{Bern}(p)$. If $s = 0$, set $I = 0$, else sample $I \sim \mathcal{N}(\mu_I, \sigma_I^2)$. Finally, determine the value of the pixel in the output image by sampling $x \sim \mathcal{N}(I, \sigma_d^2)$."
4; 4; "d"; Essay"; "NULL"; "Suppose that you are developing an optical instrument for the James Webb Space Telescope. The instrument will be used to study distant stars. Your task is to evaluate and compare different designs (e.g. different number of lenses and corresponding focal lengths). You conclude that it would probably be helpful to simulate what the resulting images would look like (without launching the instrument to space first!). You have access to a relatively large number of images, captured with a range of known designs, and would like to build a simulator that can synthesize new, realistic, images corresponding to a given design. The instrument has an image sensor of size 100 x 100 pixels. Independently of each other, each pixel corresponds to either a star, with probability $p$, or empty space, with probability $1-p$. Star pixels have different intensities distributed according to $I \sim \mathcal{N}(\mu_I,\sigma_I^2)$ while non-star pixels have zero intensity. Each pixel in the entire final image corresponds to the intensity plus some noise. The noise is Gaussian, with zero mean and variance $\sigma_d^2$ (known from the design). The noise in each pixel is independent and identically distributed. Consider the model for this with prior: $$p(I_k) = \mathcal{N}(I_k; \mu_I, \sigma_I^2), $$ and likelihood: $$ p(\mathbf{X}_k | I_k) = \prod_{i=1}^{3} \mathcal{N}(X_{ki}; I_k, \sigma_d^2) = \mathcal{N} \left( \mathbf{X}_k; \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} I_k, \begin{pmatrix} \sigma_d^2 & 0 & 0 \\ 0 & \sigma_d^2 & 0 \\ 0 & 0 & \sigma_d^2 \end{pmatrix} \right).$$ Hence, the model is a Gauss-Gauss conjugate pair, and the posterior is the Gaussian distribution given by Corollary 1 on the Formula Sheet using $$ x_a = I_k, \quad x_b = \mathbf{X}_k = (1, 4, 1)^\top, $$ $$ \mu_a = \mu_I = 3, \quad \Sigma_a = \sigma_I^2 = 4, $$ $$ A = (1, 1, 1)^\top, \quad b = 0, $$ $$ \Sigma_{b|a} = \text{diag}(\sigma_d^2) = I_{3 \times 3}, $$ namely, $$ p \left( I_k | \mathbf{X}_k = (1, 4, 1)^\top \right) = \mathcal{N}(I_k; \mu_{I_k | \mathbf{X}_k}, \Sigma_{I_k | \mathbf{X}_k}), $$ where $\mu_{I_k | \mathbf{X}_k} = 2.2$ and $\Sigma_{I_k | \mathbf{X}_k} = 0.8$. Feeling pretty good about yourself, you explain your simulator to an astrophysicist and show a histogram of samples drawn from the posterior distribution $p(I_k|\mathbf{X}_k)$. Unfortunately, the astrophysicist only needs to glance at the histogram to conclude that the estimate is unphysical: obviously, a star's intensity can't be negative. Explain the flaw in your modelling and propose a way to fix it. Use maximum 100 words."; "The main flaw is that the support of the prior $p(I)$ extends into negative values. (You could argue similarly for the likelihood since values in an image $X$ should also be nonnegative, but this is not the main complaint of the astrophycist.) A simple fix is to replace the Gaussian distribution in the prior with a nonnegative distribution, for instance the Gamma distribution. To get a similar— but guaranteed nonnegative—posterior, one can use e.g. moment matching to select the parameters of the new prior. If the likelihood is also replaced, and chosen such that the new prior is its conjugate prior, then the posterior will be available in closed form. If not, it is likely that some form of approximate inference will be necessary."