exam;question_number;sub_question;question_type;extra_material;question;answer
2;1;"a";"Multiple choice";"Null";"Please answer the following questions regarding Bayesian inference $p(x|y)=\frac{p(y|x)p(x)}{p(y)}$. Which of the following is usually the most challenging to compute? Select one alternative. 1. The likelihood $p(y|x)$. 2. The prioc p(x). 3. The marginal likelihood p(y). ";"3. The marginal likelihood $p(y)$. Likelihood $p(y|x)$ and prior $p(x)$ are given by the model. To compute marginal likelihood we need to solve the marginalization $p(y) = \int p(y|x)p(x)dx$ which is usually not tractable (note, the integral depends on likelihood and prior). "
2;1;"b";"Multiple choice";"Null";"How do we typically choose the prior distribution $p(x)$? Select one alternative. 1. Depending on the application, based on the domain/expert knowledge. 2. Depending on the application, but we can always conveniently choose Gaussian. 3. If we know that x must be negative, then we can choose the prior p(x) to be a Beta distribution.";"2. Depending on the application, based on the domain/expert knowledge."
2;1;"c";"Multiple choice";"Null";"What does $\int_0^1 p(x|y)dx$ mean? Select one alternative. 1. The probability of the event $x \in [0, 1]$ conditioned on $y$. 2. The posterior distribution evaluated at points 0 and 1. 3. The marginal likelihood $p(y)$.";"1. The probability of the event $ x \in [0, 1] $ conditioned on $ y $."
2;1;"d";"Essay";"Null";"Explain the key differences between supervised and unsupervised machine learning. Provide examples of real-world applications for each type of problem. Use maximum 200 words.";"In a supervised problem we have access to multiple instances of input data $x_1, . . . x_N$ and corresponding outputs $y_1, . . . , y_N $, which we denote as our training data. Our aim is to, based on this data, find a model $y = f(x)$ that relates the input and the output. One example is where we have data of age (input) and length (output) of various infants and, based on this data, want to find a model relating age to length. In unsupervised learning we donâ€™t have any outputs in our training data, we only have access to $x_1, . . . x_N$. Here, we instead want to find meaningful patterns in the data. One example is customer segmentation where the data points $x_1, . . . x_N$ represent various customers and we want to group them into costumers groups such that customers in the same group are more similar to each other than to those in other groups. "
2;1;"e";"Multiple choice";"Null";"Consider $y=w^2x+\cos(w)x+\epsilon$, where w is a number, $x \sim \mathcal{N}(1,1^2)$ and $\epsilon \sim \mathcal{N}(0,0.01^2)$ which is independent of $x$. What is the expected value $E[y]$? Select one alternative. 1. $E[w]x+E[\cos(w)]x$. 2. $w^2x+\cos(w)x$. 3. $w^2+\cos(w)+\epsilon$ 4. $w^2+\cos(w)$";"4. Since $E[y] = E[w^2 x + \cos(w)x + \epsilon] = w^2 E[x] + \cos(w) E[x] + E[\epsilon] = w^2 \cdot 1 + \cos(w) \cdot 1 + 0 = w^2 + \cos(w)$"
2;1;"f";"Multiple choice";"Null";"Consider $y=w^2x+\cos(w)x+\epsilon$, where w is a number, $x \sim \mathcal{N}(1,1^2)$ and $\epsilon \sim \mathcal{N}(0,0.01^2)$ which is independent of $x$. What is the covariance $\cov{x,y}$? Select one alternative. 1. $w^2 + \cos(w)$. 2. $w^2xy+\cos(w)xy$. 3. 0. 4. 0.1.";"1. $ w^2 + \cos(w) $ this is since $ \Cov[x, y] = E[(x - E[x])(y - E[y])]= E[(x - 1)(w^2 x + \cos(w)x + \epsilon - w^2 + \cos(w))]= E[(x - 1)(w^2(x - 1) + \cos(w)(x - 1) + \epsilon)]= w^2 E[(x - 1)^2] + \cos(w) E[(x - 1)^2]  = w^2 + \cos(w)$"
2;1;"g";"Multiple choice";"Null";"Consider $y=w^2x+\cos(w)x+\epsilon$, where w is a number, $x \sim \mathcal{N}(1,1^2)$ and $\epsilon \sim \mathcal{N}(0,0.01^2)$ which is independent of $x$. Suppose that $\epsilon$ is not independent of $x$, but jointly Gaussian distributed with $x$. Is the posterior distribution $p(x|y)$ still Gaussian? Select one alternative. 1. Yes. 2. No.";"1. Yes"
2;2;"a";"Multiple choice";"exam_2023_10_27_2.png";"Consider the following Bayesian network. Is this statement true or false: From the Bayesian networrk we can conclude that $A \perp \!\!\! \perp B | C$ (A is independent of B given C).";"False, they are connected along the path A-B."
2;2;"b";"Multiple choice";"exam_2023_10_27_2.png";"Consider the following Bayesian network. Is this statement true or false: From the Bayesian networrk we can conclude that $A \perp \!\!\! \perp C | B$ (A is independent of C given B). ";"True, the path A-B-C blocked at node B (observed head-to-tail)."
2;2;"c";"Multiple choice";"exam_2023_10_27_2.png";"Consider the following Bayesian network. Is this statement true or false: From the Bayesian networrk we can conclude that $B \perp \!\!\! \perp E | D$ (B is independent of E given D). ";"True, the path B-D-E blocked at node D (observed tail-to-tail)."
2;2;"d";"Multiple choice";"exam_2023_10_27_2.png";"Consider the following Bayesian network. Is this statement true or false: From the Bayesian networrk we can conclude that $A \perp \!\!\! \perp E$ (A is independent of E). ";"False, the path A-B-D-E is neither blocked at node B nor D."
2;2;"e";"Multiple choice";"exam_2023_10_27_2.png";"Consider the following Bayesian network. Is this statement true or false: From the Bayesian networrk we can conclude that $C \perp \!\!\! \perp E$ (C is independent of E). ";"True, C-B-D-E is blocked at node B (unobserved head-to-head)."
2;2;"f";"Multiple choice";"exam_2023_10_27_2.png";"Consider the following Bayesian network. Is this statement true or false: From the Bayesian networrk we can conclude that $C \perp \!\!\! \perp E | A$ (C is independent of E given A). ";"False, C-B-D-E is neither blocked at node D nor at node B (note B is unobserved head to head, but does have observed descendants namely node A)."
2;2;"g";"Essay";"exam_2023_10_27_2.png";"The graphical representation indicates that a joint distribution can be factorized $p(A, B, C, D, E)$ into a product of terms. What is the factorization encoded in the Bayesian network?";"$p(A,B,C,D,E)=p(A|B)p(B|C,D)p(C)p(D)p(E|D)$."
2;3;"a";"Essay";"exam_2023_10_27_3_a.png";"Let $X \sim p(x)$ be a random variable following a density $p(x)=\frac{1}{Z}e^{(-x\tanh(x^3))}$ where $Z$ is an unknown constant. Your task is to compute the expectation $E[e^{(-X^2)}]$ by using importance sampling. For simplicity, let us choose the proposal $q(x)=\mathcal{N}(0,3^2) to be a Gaussian with zero mean and standard deviation 3. A plot of the energy function $e^{(-x\tanh(x^3))}$ and also $q(x)@ are provided. Argue whether this $q$ is a proper proposal distribution. Use maximum 50 words.";"This question has no absolute true/false argument. The argument is true in the sense that the proposal distribution can well cover the support of the target distribution. On the other hand the argument is false in the sense that the proposal does not efficiently explore the shape of the target distribution. "
2;3;"b";"Programming";"code_2023_10_27.py";"Let $X \sim p(x)$ be a random variable following a density $p(x)=\frac{1}{Z}e^{(-x\tanh(x^3))}$ where $Z$ is an unknown constant. Your task is to compute the expectation $E[e^{(-X^2)}]$ by using importance sampling. For simplicity, let us choose the proposal $q(x)=\mathcal{N}(0,3^2) to be a Gaussian with zero mean and standard deviation 3. Provide the code for this. Make sure your code is readable and comment any nontrivial steps.";"code_answer_2023_10_27.py"
2;3;"c";"Essay";"Null";"Explain what the burn-in period of Gibbs sampling is. Use maximum 75 words.";"The burn-in period refers to the stage of an MCMC chain before it has converged to the invariant distribution up to a tolerated error. We commonly throw away the burn-in samples"
2;3;"d";"Essay";"Null";"How is Gibbs sampling compared to importance sampling? What are their pros and cons? Use maximum 150 words.";" Gibbs sampling shines when we can explicitly factorise the target distribution by their conditionals. On the other hand, Gibbs is hard to simulate when we cannot sample the conditionals. Importance sampling is easy to implement, and is useful when we have no access to the normalising constant. However, it is not always obvious how to choose an efficient proposal distribution."