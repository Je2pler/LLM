,exam,question_number,sub_question,question_type,extra_material,question,answer,response
0,1,1,a,Essay,Null,Explain the common objectives shared by Bayesian supervised machine learning methods and non-Bayesian supervised machine learning methods.,Both Bayesian and non-Bayesian supervised machine learning methods share the common objective of making predictions or classifications based on input data. They aim to learn patterns and relationships from labeled data to generalize well to unseen instances.,
1,1,1,b,Essay,Null,"Contrast the representation of models in Bayesian machine learning with that of non-Bayesian approaches, highlighting key differences in terms of parameters. Use linear regression and Bayesian linear regression as examples","Bayesian machine learning models represent uncertainty explicitly through probability distributions over parameters. In contrast, non-Bayesian approaches often use point estimates for parameters.",
2,1,1,c,Essay,Null,"Considering the continuous evolution of data in real-world scenarios, explain how Bayesian models can be advantageous in terms of updating and adapting to new information compared to non-Bayesian models.","Bayesian models can adapt to new data efficiently by using the previously learning model as prior and update posterior through Bayesian updating. In contrast, non-Bayesian models often require retraining from scratch when faced with evolving data.",
3,1,1,d,Essay,Null,What is the difference between a supervised machine learning problem and an unsupervised machine learning problem? Use maximum 75 words.,"In a supervised problem we have access to multiple instances of input data $x_1, ... x_N$ and corresponding outputs $y_1, ... , y_N$, which we denote as our training data. Our aim is to, based on this data, find a model $y = f(x)$ that relates the input and the output. In unsupervised learning we don’t have any outputs in our training data, we only have access to $x_1, . . . x_N$. Here, we instead want to find meaningful patterns in the data.",
4,1,1,e,Essay,Null,"Consider the measurement model: $y_n = x + \epsilon_n, \quad n=1,...,N$, where all noise terms $\epsilon_n ~ \mathcal{N}(0,1^2)$, what is the mean $\mu_{x|y}$ and variance $\sigma^2_{x|y}$ of the posterior? $p(x|y_1, y_2) = \mathcal{N}(x;\mu_{x|y}, \sigma^2_{x|y}$)","The likelihood is: \[p(y_1, y_2 \mid x) = \mathcal{N}\left(\begin{bmatrix} y_1 \\ y_2 \end{bmatrix}; \begin{bmatrix} 1 \\ 1 \end{bmatrix} x, \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}\right)\]. Together with the expressions for the prior: $p(x) \sim \mathcal{N}(0, 1^2)$, Corollary 1 gives us the Gaussian posterior: \[p(x \mid y_1, y_2) = \mathcal{N}\left(x; \mu_{x \mid y}, \sigma_{x \mid y}^2\right)\] \text{where}\[\sigma_{x \mid y}^2 = \left( 1^{-1} + \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}^{-1} \begin{bmatrix} 1 \\ 1 \end{bmatrix}\right)^{-1} = \frac{2}{3},\]\[\mu_{x \mid y} = \frac{2}{3} \left( 0 + \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}^{-1} \left(\begin{bmatrix} 1 \\ -2 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)\right) = -\frac{1}{6}\]",
5,1,2,a,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp B$ (text A is independent of B),False: the path A-C-B is not blocked at C,
6,1,2,b,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp E$ (text A is independent of E),True: the path A-C-E is blocked at C (unobserved head-to-head),
7,1,2,c,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp B | D$ (text A is independent of B given D),False: the path A-C-B is still not blocked at C (observing D does not change this),
8,1,2,d,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp B | C$ (text A is independent of B given C),True: the path A-C-B is blocked at C (observed head-to-tail),
9,1,2,e,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp E | B$ (text A is independent of E given B),False: the path A-C-E is unblocked (unobserved head-to-head but with observed descendants at B),
10,1,2,f,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp D | E$ (text A is independent of D given E),True: The path A-C-E-D is blocked both at C (unobserved head-to-head) and E (observed head-to-tail),
11,1,2,g,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp D | C$ (text A is independent of D given C),False: The path A-C-E-D is neither blocked at C nor E.,
12,1,2,h,Multiple Choice,BN240128.png,"Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp D | C, E$ (text A is independent of D given C and D)",True: The path A-C-E-D is blocked at E (observed head-to-tail),
13,1,2,i,Essay,BN240128.png,"Consider the Bayesian network in the image provided. The graphical representation indicates that a joint distribution can be fatorized p(A,B,C,D,E) into a product of terms. What is the factorization encoded in the Bayesian network. $p(A,B,C,D,E)","$p(A, B, C, D, E) = p(A)p(B|C)p(C|A, E)p(D)p(E|D)$",
14,1,2,i,Essay,BN240128.png,"Consider the Bayesian network in the image provided. Consider the factorization encoded in the Bayesian network. $p(A,B,C,D,E). Consider all five random variables A, B, C, D, E to be binary. What is the minimum number of parameters needed to specify the joint distribution $p(A, B, C, D, E)$ given the factorization from the image provided.","To specify $p(A)$,$$p(A = 1) = \alpha, \quad p(A = 0) = 1 - \alpha $$ we need 1 parameter.To specify $p(B | C)$,$$p(B = 1 | C = 0) = \beta_1, \quad p(B = 0 | C = 0) = 1 - \beta_1$$$$p(B = 1 | C = 1) = \beta_2, \quad p(B = 0 | C = 1) = 1 - \beta_2$$ we need 2 parameters. To specify $p(C | A, E)$,$$p(C = 1 | A = 0, E = 0) = \gamma_1, \quad p(C = 0 | A = 0, E = 0) = 1 - \gamma_1$$$$p(C = 1 | A = 1, E = 0) = \gamma_2, \quad p(C = 0 | A = 1, E = 0) = 1 - \gamma_2$$$$p(C = 1 | A = 0, E = 1) = \gamma_3, \quad p(C = 0 | A = 0, E = 1) = 1 - \gamma_3 $$ $$p(C = 1 | A = 1, E = 1) = \gamma_4, \quad p(C = 0 | A = 1, E = 1) = 1 - \gamma_4$$we need 4 parameters. Similarly for $p(D)$ and $p(E | D)$ we need in total $1 + 2 + 4 + 1 + 2 = 10$ parameters.",
15,1,2,i,Essay,BN240128.png,"Consider the Bayesian network in the image provided. Consider the factorization encoded in the Bayesian network. $p(A,B,C,D,E). Consider all five random variables A, B, C, D, E to be binary. What is the minimum number of parameters needed to specify the joint distribution $p(A, B, C, D, E)$ if no assumption about its factorization is made.","To specify $p(A, B, C, D, E)$ we need to specify all combinations of the joint $$p(A = 1, B = 1, C = 1, D = 1, E = 1) = p_1,$$$$p(A = 0, B = 1, C = 1, D = 1, E = 1) = p_2,$$$$\cdots,$$$$p(A = 0, B = 0, C = 0, D = 0, E = 1) = p_{31},$$$$p(A = 0, B = 0, C = 0, D = 0, E = 0) = 1 - p_1 - \cdots - p_{31}$$That means we need $2^5 - 1 = 31$ parameters. \textbf{Alternative solution:} You can also consider the full factorization of the joint $p(A, B, C, D, E) = p(A)p(B|A)p(C|A, B)p(D|A, B, C)p(E|A, B, C, D)$ which requires $1+2+4+8+16=31$ parameters by following the same line of arguments in the previous exercise.",
16,1,3,a,Programming,code_2024_01_28_template_is.py,"Let $X ~ p(x)$ be a random variable following a density $p(x)=\frac{1}{Z}exp(-x^4)$ where Z is an unknown constant. Your task is to compute the variance of X, i.e., computing the expectation $E[(X-E[X])^2]$ by using importance sampling. For simplicity, let us choose the proposal $q(x)=\mathcal{N}(0,\sigma^2)$ to be a Gaussian with zero mean and standard deviation $\sigma$. Examine the code provided that plots $p(x)$ and $q(x)$. Select the parameter sigma to get a proper proposal distribution. Argue for your choice of sigma. In particular, argue what problem you run into if you pick a sigma: (i) too small (ii) too large.","(i) $\sigma$ should not be too small, such that the proposal distribution cannot well cover the support of the target distribution. That would lead to regions of the target distribution not containing any samples. (ii) $\sigma$ should not be too big, since many samples would be far away from the target distribution with a consequence that the corresponding weight $w_i \approx 0$ and hence not contributing to the estimation. $\sigma = 1$ seems to be a reasonable pick since the support of proposal distribution is well covered, but also not too big in comparison to the target such that particles would be waisted. ","```python
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

def p(x):
  """"""The unnormalised target density.""""""
  return np.exp(-x**4)

def q(x, sigma):
  """"""The proposal density.""""""
  return stats.norm.pdf(x, 0, sigma)

# Standard deviation of proposal
sigma = 1

# Plot p(x) and q(x)
xs = np.linspace(-7, 7, 1000)
plt.plot(xs, p(xs), label='Target p(x)')
plt.plot(xs, q(xs, sigma), label='Proposal q(x)')
plt.legend()
plt.show()

# Importance sampling
def importance_sampling(p, q, sigma, num_samples=1000):
  """"""Importance sampling to estimate the variance of a random variable.""""""

  # Draw samples from the proposal distribution q(x)
  samples = stats.norm.rvs(loc=0, scale=sigma, size=num_samples)

  # Compute the importance weights
  weights = p(samples) / q(samples, sigma)
  normalised_weights = weights / np.sum(weights)

  # Compute the mean and variance
  mean = np.sum(normalised_weights * samples)
  variance = np.sum(normalised_weights * (samples - mean)**2)

  return mean, variance

# Compute the mean and variance using importance sampling
mean, variance = importance_sampling(p, q, sigma)

print(f""Estimated mean: {mean}"")
print(f""Estimated variance: {variance}"")
```

**Choosing sigma**

The choice of sigma is important for the performance of the importance sampling method. It controls the
breadth of the proposal distribution. 

* **Too small sigma**: The proposal distribution will be very narrow and the acceptance rate of
candidates will be low. This is because only a small number of candidates will have a
probability under the proposal distribution that is significantly higher than that of the
current sample. This leads to slow exploration of the state space.

* **Too large sigma**: The proposal distribution will be very broad and may not reflect the
shape of the target distribution well. This means that some candidates drawn from the
proposal distribution may have a very low probability under the target distribution.
This leads to a poor estimate of the expectation, since these candidates will be assigned
a very small importance weight.

In the code provided, we have chosen sigma = 1, which is a reasonable balance. The
proposal distribution is broad enough to cover the important regions of the target
distribution, but not so broad that it assigns negligible weight to many candidates.


Here's the explanation for the provided code:

- **`p(x)`**: This function defines the unnormalised target density.
- **`q(x, sigma)`**: This function defines the Gaussian proposal distribution with mean 0
and standard deviation sigma.
- **`importance_sampling(p, q, sigma, num_samples=1000)`**: This function implements
the importance sampling algorithm. It takes the target density `p`, the proposal
density `q`, the standard deviation of the proposal `sigma`, and the number of samples
`num_samples` as arguments. 
- **`mean, variance = importance_sampling(p, q, sigma)`**: This line calls the
importance sampling function to compute the mean and variance of the random
variable X using the specified `p`, `q`, and `sigma`.
- **`print(f""Estimated mean: {mean}"")`**: This line prints the estimated mean of X
obtained from the importance sampling.
- **`print(f""Estimated variance: {variance}"")`**: This line prints the estimated variance
of X obtained from the importance sampling.

The code first plots the target density `p(x)` and the proposal density `q(x)` to
visualise their shapes and how well they match. Then, it performs importance
sampling to estimate the mean and variance of the random variable X. Finally, it
prints the estimated mean and variance. 
"
17,1,3,b,Programming,code_2024_01_28_template_is.py,Implement a python routine that computes $E[(X-E[X])^2]$ with 10000 samples. Use the code provided as a base. ,code_2024_01_28_is.py,"```python
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

def p(x):
  """"""The unnormalised target density.""""""
  return np.exp(-x**4)

def q(x, sigma):
  """"""The proposal density.""""""
  return stats.norm.pdf(x, 0, sigma)

# Standard deviation of proposal
sigma = 1

# Plot p(x) and q(x)
xs = np.linspace(-7, 7, 1000)
plt.plot(xs, p(xs), label='Target p(x)')
plt.plot(xs, q(xs, sigma), label='Proposal q(x)')
plt.legend()
plt.show()

# Importance sampling
def importance_sampling(p, q, sigma, num_samples=10000):
  """"""Importance sampling to estimate the variance of a random variable.""""""

  # Draw samples from the proposal distribution q(x)
  samples = stats.norm.rvs(loc=0, scale=sigma, size=num_samples)

  # Compute the importance weights
  weights = p(samples) / q(samples, sigma)
  normalised_weights = weights / np.sum(weights)

  # Compute the mean and variance
  mean = np.sum(normalised_weights * samples)
  variance = np.sum(normalised_weights * (samples - mean)**2)

  return mean, variance

# Compute the mean and variance using importance sampling
mean, variance = importance_sampling(p, q, sigma)

print(f""Estimated mean: {mean}"")
print(f""Estimated variance: {variance}"")
```

This code first defines the functions `p(x)` and `q(x, sigma)` for the target
and proposal distributions, respectively. Then, it plots these distributions to
visualize their shapes and how well they match.

The `importance_sampling(p, q, sigma, num_samples=10000)` function
implements the importance sampling algorithm. It takes the target density `p`,
the proposal density `q`, the standard deviation of the proposal `sigma`, and
the number of samples `num_samples` as arguments. The function draws
samples from the proposal distribution, computes the importance weights,
normalizes the weights, and then calculates the mean and variance of the
random variable X using the weighted samples.

Finally, the code calls the `importance_sampling` function to compute the
mean and variance of X using the specified `p`, `q`, and `sigma`, and then
prints the estimated mean and variance.

The code provided is a basic implementation of importance sampling and can be
used as a starting point for more advanced applications. In particular, one
could experiment with different proposal distributions, such as a mixture of
Gaussians, to improve the accuracy of the estimates. One could also try
different methods for collapsing the mixture of Gaussians, such as the
`mix2mix` function provided in the course book.
"
18,1,3,c,Programming,code_2024_01_28_template_gibbs.py,"A Gibbs sampler is considered. Consider the following two-dimensional Gaussian random variable $\begin{bmatrix}x \\ y\end{bmatrix} ~ \mathcal{N}(\begin{bmatrix}5 \\ -5\end{bmatrix}, \begin{bmatrix}4 & -1\\ -1 & 1\end{bmatrix})$ which has the following conditional distributions $p(x|y)=\mathcal{N}(x;\mu_{x|y},\sigma^2_{x|y})$ where $\mu_{x|y}=-y$, $\sigma^2_{x|y}=3$ and $p(y|x)=\mathcal{N}(y;\mu_{y|x}, \sigma^2_{y|x})$, where $\mu_{y|x}=-0.25x-3.75$, $\sigma^2_{y|x}=0.75$. Implement a Gibbs sampler for $p(x,y)$ by sampling from its conditionals $p(x|y)$ and $p(y|x)$. Draw L=1000 samples and plot the sample trajectories. Make sure your code is readable and comment any non-trivial steps. Plot the sample trajectories. Use the code provided as a base. ", ,"```python
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

L = 1000  # Number of samples
x = np.zeros(L)
y = np.zeros(L)

# Initialisation
x[0] = 5  # Start with the mean of the x distribution
y[0] = -5 # Start with the mean of the y distribution

# Define the conditional distributions
def p_x_given_y(y, mu_x_given_y, sigma_x_given_y):
  """"""Conditional distribution p(x|y)""""""
  return stats.norm.pdf(y, loc=mu_x_given_y, scale=sigma_x_given_y)

def p_y_given_x(x, mu_y_given_x, sigma_y_given_x):
  """"""Conditional distribution p(y|x)""""""
  return stats.norm.pdf(x, loc=mu_y_given_x, scale=sigma_y_given_x)

# Gibbs sampling
for i in range(1, L):
  # Sample x from p(x|y)
  mu_x_given_y = -y[i-1]
  sigma_x_given_y = 3
  x[i] = stats.norm.rvs(loc=mu_x_given_y, scale=sigma_x_given_y)

  # Sample y from p(y|x)
  mu_y_given_x = -0.25*x[i] - 3.75
  sigma_y_given_x = 0.75
  y[i] = stats.norm.rvs(loc=mu_y_given_x, scale=sigma_y_given_x)

# Plot the results
fig, (ax1, ax2) = plt.subplots(2)
ax1.set_title('Sample trajectories')
ax1.plot(x, label='x')
ax1.plot(y, label='y')
ax1.set(xlabel='Sample number')
ax1.legend()
ax2.plot(x, y, '.')
ax2.set_title('Samples')
ax2.set(xlabel='x', ylabel='y')
ax2.set_aspect('equal', 'box')
fig.tight_layout()
plt.show()
```
In this code:

1. We initialize the Gibbs sampler by setting the initial values of x and y to their means.

2. The code then iterates through the samples, drawing a new sample for x
from p(x|y) and a new sample for y from p(y|x) using the specified conditional
distributions.

3. The conditional distributions are defined in the functions
`p_x_given_y` and `p_y_given_x`.

4. The code then plots the sample trajectories for x and y to visualize the
behavior of the Gibbs sampler. 

The plot shows the sample trajectories for x and y. As you can see, the
samples are not independent, but they do explore the joint distribution
`p(x,y)`. The samples are also clustered around the mean of the
distribution.

This is a basic implementation of a Gibbs sampler, and it is important
to note that there are many different ways to implement it. In particular,
one could experiment with different initial values for x and y, different
proposal distributions, and different methods for collapsing the mixture of
Gaussians. This would lead to different sample trajectories and different
estimates of the expectation.


"
19,1,4,a,Essay,Null,"Denote the number of goals made by a certain team in the match $i$ as $y_i$. We assume $y_i$ to be Poisson distributed: $p(y_i = k | \lambda) = \text{Pois}(k; \lambda) = \frac{1}{k!} \lambda^k e^{-\lambda} \quad \text{for} \quad k = 0, 1, 2, \dots, \quad \text{and} \quad \lambda > 0$ where $\lambda$ is a parameter determining how many goals the team does on average during a match. The team plays $\N$ matches with the number of goals being $y_1, \dots, y_N$ for these $N$ matches. We assume that all $y_1, \dots, y_N$ are conditionally independent given $\lambda$. For the parameter $\lambda$, we choose the so-called Gamma distribution: $p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \quad \text{for} \quad \lambda > 0, \quad \alpha, \beta > 0$In an attempt to improve the model used in the mini-project, we want to exploit the information about the number of goals made in a certain match.$ Here, $\Gamma(\alpha)$ denotes the Gamma function. Describe the dependencies between the random variables \(\lambda, y_1, \dots, y_N\) by writing down the corresponding factorization of the joint distribution \(p(\lambda, y_1, \dots, y_N)\).","p(\lambda, y_1, \dots, y_N) = p(y_1 \mid \lambda) \cdot \dots \cdot p(y_N \mid \lambda) p(\lambda)","The dependencies between the variables are shown in the following belief network: 

<start_of_image> схема сети Байеса 
*  $p(\lambda, y_1, \dots, y_N) = p(\lambda) \prod_{i=1}^N p(y_i | \lambda)$ 

The joint distribution can be expressed as:

$p(\lambda, y_1, \dots, y_N) = \text{Gamma}(\lambda; \alpha, \beta) \prod_{i=1}^N \text{Pois}(y_i; \lambda)$ 

Since the $y_i$ are conditionally independent given $\lambda$, their individual distributions are multiplied. The overall distribution then depends on the parameter $\lambda$ and the Gamma prior $p(\lambda)$ which also affects the final distribution of each $y_i$ and, therefore, the overall distribution $p(\lambda, y_1, \dots, y_N)$.

"
20,1,4,b,Essay,Null,"Denote the number of goals made by a certain team in the match $i$ as $y_i$. We assume $y_i$ to be Poisson distributed: $p(y_i = k | \lambda) = \text{Pois}(k; \lambda) = \frac{1}{k!} \lambda^k e^{-\lambda} \quad \text{for} \quad k = 0, 1, 2, \dots, \quad \text{and} \quad \lambda > 0$ where $\lambda$ is a parameter determining how many goals the team does on average during a match. The team plays $\N$ matches with the number of goals being $y_1, \dots, y_N$ for these $N$ matches. We assume that all $y_1, \dots, y_N$ are conditionally independent given $\lambda$. For the parameter $\lambda$, we choose the so-called Gamma distribution: $p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \quad \text{for} \quad \lambda > 0, \quad \alpha, \beta > 0$In an attempt to improve the model used in the mini-project, we want to exploit the information about the number of goals made in a certain match.$ Here, $\Gamma(\alpha)$ denotes the Gamma function. We want to compute the posterior $p(\lambda|y_1,\dots, y_N) based on goals from $N$ matches. i) Write down Bayes' formula for this problem. ii) What are observed and latent variables for this problem? iii) What is the prior and likelihood for this problem?","\begin{enumerate}\item[i)] \[p(\lambda \mid y_1, \dots, y_N) = \frac{p(y_1, \dots, y_N \mid \lambda) p(\lambda)}{p(y_1, \dots, y_N)}\]\item[ii)] Latent variable: $\lambda$. Observed variables: $y_1, \dots, y_N$\item[iii)] Prior distribution: $p(\lambda)$. Likelihood: $p(y_1, \dots, y_N \mid \lambda)$\end{enumerate}","i) Bayes' formula for this problem is:

$p(\lambda|y_1, \dots, y_N) = \frac{p(y_1, \dots, y_N | \lambda)p(\lambda)}{p(y_1, \dots, y_N)}$

where:

* $p(\lambda|y_1, \dots, y_N)$ is the posterior distribution of $\lambda$ given the data $y_1, \dots, y_N$.
* $p(y_1, \dots, y_N | \lambda)$ is the likelihood of the data $y_1, \dots, y_N$ given $\lambda$.
* $p(\lambda)$ is the prior distribution of $\lambda$.
* $p(y_1, \dots, y_N)$ is the marginal likelihood of the data, which can be computed as: $p(y_1, \dots, y_N) = \int_0^\infty p(y_1, \dots, y_N | \lambda)p(\lambda) d\lambda$. 

ii)  The observed variables are the number of goals scored in each match $y_i$. The latent variable is the parameter $\lambda$, which is not directly observed.

iii) The likelihood of the data is:

$p(y_1, \dots, y_N | \lambda) = \prod_{i=1}^N \text{Pois}(y_i; \lambda)$

since each $y_i$ is conditionally independent given $\lambda$.
The prior for the parameter $\lambda$ is given by the Gamma distribution:

$p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda}$.

The Gamma distribution is a common choice for priors on parameters that are positive and are known to be non-negative, such as the rate parameter of a Poisson distribution, and is conjugate to the Poisson distribution.

"
21,1,4,c,Essay,Null,"Denote the number of goals made by a certain team in the match $i$ as $y_i$. We assume $y_i$ to be Poisson distributed: $p(y_i = k | \lambda) = \text{Pois}(k; \lambda) = \frac{1}{k!} \lambda^k e^{-\lambda} \quad \text{for} \quad k = 0, 1, 2, \dots, \quad \text{and} \quad \lambda > 0$ where $\lambda$ is a parameter determining how many goals the team does on average during a match. The team plays $\N$ matches with the number of goals being $y_1, \dots, y_N$ for these $N$ matches. We assume that all $y_1, \dots, y_N$ are conditionally independent given $\lambda$. For the parameter $\lambda$, we choose the so-called Gamma distribution: $p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \quad \text{for} \quad \lambda > 0, \quad \alpha, \beta > 0$In an attempt to improve the model used in the mini-project, we want to exploit the information about the number of goals made in a certain match.$ Here, $\Gamma(\alpha)$ denotes the Gamma function. We want to compute the posterior $p(\lambda|y_1,\dots, y_N) based on goals from $N$ matches. Show that the posterior is a Gamma distribution $p(\lambda|y_1\dots, y_N) = \text{Gamma}(\lambda;\alpha^*,\Beta^*)$ and determine its parameters $\alpha^*, \Beta^*$ as a function of $\alpha, \Beta, y_1\dots,y_N$","\begin{equation}\begin{aligned}p(\lambda | y_1, ..., y_N) &\propto p(y_1, ..., y_N | \lambda)p(\lambda) \\&\propto \lambda^{\prod_{i=1}^Ny_i}e^{-N\lambda}\lambda^{\alpha-1}e^{-\beta\lambda} \\&= \lambda^{\prod_{i=1}y_i+\alpha-1}e^{-(\beta+N)\lambda} \\&= \lambda^{\alpha^*-1}e^{-\beta^*\lambda}\\&\propto \text{Gamma}(\lambda;\alpha^*,\beta^*)\end{aligned}\end{equation}This is proportional to a Gamma distribution with \begin{equation*}    \alpha^* = \alpha + \sum_{i=1}^Ny_i\quad \beta^* = \beta + N\end{equation*}Hence, the Gamma distribution is a conjugate prior to the Poisson distribution where the parameters of the posterior are given above.","To compute the posterior distribution $p(\lambda|y_1,\dots, y_N)$, we use Bayes' theorem:

$$p(\lambda|y_1,\dots, y_N) = \frac{p(y_1,\dots, y_N | \lambda)p(\lambda)}{p(y_1,\dots, y_N)}.$$

Since the $y_i$ are conditionally independent given $\lambda$, we can write the likelihood as:

$$p(y_1,\dots, y_N | \lambda) = \prod_{i=1}^N \text{Pois}(y_i; \lambda) = \prod_{i=1}^N \frac{1}{y_i!} \lambda^{y_i} e^{-\lambda}.$$

The prior distribution for $\lambda$ is given by the Gamma distribution:

$$p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda}.$$

Putting everything together, we get:

$$p(\lambda|y_1,\dots, y_N) \propto \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \prod_{i=1}^N \frac{1}{y_i!} \lambda^{y_i} e^{-\lambda} = \frac{1}{\prod_{i=1}^N y_i!} \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha + \sum_{i=1}^N y_i - 1} e^{-(\beta + N) \lambda}.$$

This is proportional to a Gamma distribution with parameters $\alpha^* = \alpha + \sum_{i=1}^N y_i$ and $\beta^* = \beta + N$. Therefore, the posterior is:

$$p(\lambda|y_1\dots, y_N) = \text{Gamma}(\lambda;\alpha^*,\Beta^*) = \frac{1}{\Gamma(\alpha^*)} (\beta^*)^{\alpha^*} \lambda^{\alpha^* - 1} e^{-(\beta^*) \lambda}$$

with $\alpha^* = \alpha + \sum_{i=1}^N y_i$ and $\beta^* = \beta + N$.

This result demonstrates the conjugacy of the Gamma and Poisson distributions. In this case, the prior and posterior distributions are both Gamma distributions, with updated parameters. This makes updating our beliefs about $\lambda$ using the Gamma prior computationally tractable.

"
22,1,4,d,Essay,Null,"Denote the number of goals made by a certain team in the match $i$ as $y_i$. We assume $y_i$ to be Poisson distributed: $p(y_i = k | \lambda) = \text{Pois}(k; \lambda) = \frac{1}{k!} \lambda^k e^{-\lambda} \quad \text{for} \quad k = 0, 1, 2, \dots, \quad \text{and} \quad \lambda > 0$ where $\lambda$ is a parameter determining how many goals the team does on average during a match. The team plays $\N$ matches with the number of goals being $y_1, \dots, y_N$ for these $N$ matches. We assume that all $y_1, \dots, y_N$ are conditionally independent given $\lambda$. For the parameter $\lambda$, we choose the so-called Gamma distribution: $p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \quad \text{for} \quad \lambda > 0, \quad \alpha, \beta > 0$In an attempt to improve the model used in the mini-project, we want to exploit the information about the number of goals made in a certain match.$ Here, $\Gamma(\alpha)$ denotes the Gamma function. We want to compute the posterior $p(\lambda|y_1,\dots, y_N) based on goals from $N$ matches. Based on the Gamma posterior $p(\lambda|y_1,\dots,y_N) = $ Gamma $(\lambda;\alpha^*,\beta^*)$, you now want to find the predictive distribution $p(y^*|y_1,\dots,y_N)$, i.e., the distribution of the number of goals in a new match $y^*$ conditioned on the number of goals scored in all previous $N$ matches. Show that the predictive distribution is a negative binomial distribution $p(y^*=k|y_1,\dots,y_N) = \text{Neg-bin}(k;r,p) = \frac{\Gamma(k+r)}{k!\Gamma(r)}(1-p)^kp^r$ and determine the parameters $p$ and $r$ above.","The predictive distribution $p(y^*|y_1,\dots,y_N)$ we get by multiplying the likelihood for the new datapoint $y^*$ with the posterior and marginalize out $\lambda$. \begin{equation}\begin{aligned}p(y^*|y_1,\dots,y_N) &= \int_0^{\infty} p(y^*|\lambda)p(\lambda|y_1,\dots,y_N)d\lambda \\ &= \int_0^{\infty}\frac{\lambda^{y^*}e^{-\lambda}}{y^*!}\frac{(\beta^*)^{\alpha^*}\lambda^{\alpha^*-a}e^{-\beta^*\lambda}}{\Gamma(\alpha^*)} \\&=\frac{(\beta^*)^{\alpha^*}\Gamma(y^*+\alpha^*)}{(\beta^*+1)^{y^*+\alpha^*}y^*!\Gamma(\alpha^*)} \int_0^{\infty} \frac{(\beta^*+1)^{y^*+\alpha^*}\lambda^{y^*+\alpha^*-1}e^{-(\beta^*+1)\lambda}}{\Gamma(y^*+\alpha^*)} d\lambda\\&= \frac{(\beta^*)^{\alpha^*}\Gamma(y^*+\alpha^*)}{(\beta^*+1)^{y^*+\alpha^*}y^*!\Gamma(\alpha^*)} \int_0^{\infty}\text{Gamma}(\lambda;y^*+\alpha^*,\beta^*+1)d\lambda \\&=\frac{(\beta^*)^{\alpha^*}\Gamma(y^*+\alpha^*)}{(\beta^*+1)^{y^*+\alpha^*}y^*!\Gamma(\alpha^*)}\\&=\frac{\Gamma(y^*+\alpha^*}{y^*!\Gamma(\alpha^*)}\left( \frac{\beta^*}{\beta^*+1}\right)^{\alpha^*} \left(1-\frac{\beta^*}{\beta^*+1} \right)^{y^*} \\&= \textbf{Neg-bin}\left(y^*;\alpha^*,\frac{\beta^*}{\beta^*+1} \right)\end{aligned}\end{equation}","We want to compute the predictive distribution $p(y^*|y_1, \dots, y_N)$, which is the distribution of the number of goals in a new match $y^*$ given the number of goals scored in all previous $N$ matches.

We can achieve this by marginalizing over the parameter $\lambda$ using the Gamma posterior distribution $p(\lambda|y_1, \dots, y_N) = \text{Gamma}(\lambda; \alpha^*, \beta^*)$:

$$p(y^*|y_1,\dots, y_N) = \int_0^\infty p(y^*|\lambda) p(\lambda|y_1,\dots,y_N) d\lambda.$$

The likelihood of observing $y^*$ goals in a new match given $\lambda$ is given by the Poisson distribution:

$$p(y^*|\lambda) = \text{Pois}(y^*;\lambda) = \frac{1}{y^*!} \lambda^{y^*} e^{-\lambda}.$$

Substituting the Gamma posterior and the Poisson likelihood into the marginalization formula, we get:

$$p(y^*|y_1,\dots, y_N) = \int_0^\infty \frac{1}{y^*!} \lambda^{y^*} e^{-\lambda} \frac{1}{\Gamma(\alpha^*)} (\beta^*)^{\alpha^*} \lambda^{\alpha^* - 1} e^{-(\beta^*) \lambda} d\lambda.$$

Simplifying the expression, we have:

$$p(y^*|y_1,\dots, y_N) = \frac{(\beta^*)^{\alpha^*}}{y^*! \Gamma(\alpha^*)} \int_0^\infty \lambda^{\alpha^* + y^* - 1} e^{-(\beta^* + 1) \lambda} d\lambda.$$

Recognizing that the integral is the normalizing constant of a Gamma distribution with parameters $\alpha^* + y^*$ and $\beta^* + 1$, we can write:

$$p(y^*|y_1,\dots, y_N) = \frac{(\beta^*)^{\alpha^*}}{y^*! \Gamma(\alpha^*)} \frac{\Gamma(\alpha^* + y^*)}{(\beta^* + 1)^{\alpha^* + y^*}}.$$

This expression can be rewritten in terms of the negative binomial distribution, using the following relation between the Gamma and negative binomial distributions:

$$\text{Neg-bin}(k;r,p) = \frac{\Gamma(k+r)}{k!\Gamma(r)}(1-p)^kp^r = \frac{1}{k!} \frac{\Gamma(k+r)}{\Gamma(r)} \left(\frac{p}{1-p}\right)^r \left( \frac{1-p}{p} \right)^k.$$

Therefore, we can express the predictive distribution as:

$$p(y^*=k|y_1,\dots,y_N) = \text{Neg-bin}(k;r,p) = \frac{\Gamma(k+r)}{k!\Gamma(r)}(1-p)^kp^r$$

where $r = \alpha^*$ and $p = \frac{\beta^*}{\beta^* + 1}$.

This result shows that the predictive distribution of $y^*$ given the observed data is a negative binomial distribution. The negative binomial distribution is a suitable choice for modelling count data when the number of trials is unknown or variable.  In this case, the number of trials is the expected number of goals scored in a new match, which is unknown. The parameter $r$ represents the number of successes needed to stop the process, which is given by the posterior parameter $\alpha^*$, and the parameter $p$ is the probability of success in each trial, which is related to the posterior parameter $\beta^*$. 



"
