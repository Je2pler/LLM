,exam,question_number,sub_question,question_type,extra_material,question,answer,response
0,1,1,a,Essay,Null,Explain the common objectives shared by Bayesian supervised machine learning methods and non-Bayesian supervised machine learning methods.,Both Bayesian and non-Bayesian supervised machine learning methods share the common objective of making predictions or classifications based on input data. They aim to learn patterns and relationships from labeled data to generalize well to unseen instances.,"Both the Bayesian and non-Bayesian supervised learning methods share the common objective of minimizing the loss function, or equivalently maximizing the utility function. A loss function measures the error between the predicted and true class labels, and the aim of learning is to find parameters of a model that minimize the expected loss over the data distribution.

In Bayesian methods, the loss function is incorporated into the prior, while in non-Bayesian methods, it is explicitly specified as a penalty term added to the objective function. 

While the specific strategies employed by the two approaches may differ, their ultimate goal remains the same: to produce a model that minimizes the error in predicting class labels for unseen data.
"
1,1,1,b,Essay,Null,"Contrast the representation of models in Bayesian machine learning with that of non-Bayesian approaches, highlighting key differences in terms of parameters. Use linear regression and Bayesian linear regression as examples","Bayesian machine learning models represent uncertainty explicitly through probability distributions over parameters. In contrast, non-Bayesian approaches often use point estimates for parameters.","In Bayesian machine learning, models are represented by a probability distribution over parameters, while in non-Bayesian approaches, models are typically represented by a single set of point estimates for the parameters.

Let's illustrate this with linear regression as an example.

**Non-Bayesian Linear Regression**

* **Model:** y = wTx + b, where w is the weight vector, b is the bias, and x is the input vector.
* **Objective:** Minimize the sum squared error between the predicted outputs and the true outputs.
* **Parameter Estimation:** The optimal w and b are found by solving the normal equations using maximum likelihood estimation, which provides a single point estimate for the parameters.

**Bayesian Linear Regression**

* **Model:**  y = wTx + b, where w is the weight vector, b is the bias, and x is the input vector. 
* **Objective:**  Maximize the marginal likelihood p(D|α, β) which is the likelihood of the data given the hyperparameters α and β.
* **Parameter Estimation:** In this case, we obtain a posterior distribution over the parameters, p(w|D, α, β), which reflects the uncertainty about the parameter values given the data and the prior beliefs. The hyperparameters α and β control the flexibility of the prior on the weights and the level of noise on the observations, respectively.

**Key Differences**

* **Representation:** Bayesian models are represented by distributions over parameters, while non-Bayesian methods provide a single point estimate for the parameters.
* **Uncertainty:**  Bayesian models explicitly acknowledge the uncertainty in parameter estimates, while non-Bayesian methods ignore this uncertainty.
* **Hyperparameters:** Bayesian models use hyperparameters (α, β) to control the flexibility of the prior on the parameters and the level of noise on the observations. Non-Bayesian models do not typically use hyperparameters.

**Example:** In Bayesian linear regression, the model is represented by the posterior distribution p(w|D, α, β), which is a Gaussian with mean m and covariance S, as shown in section(18.1.1). The mean prediction for an input x is then given by:
  ¯f(x) ≡ ∫f(x; w)p(w|D, Γ)dw = mTφ (x).

In contrast, in non-Bayesian linear regression, the model is represented by a single point estimate of the weight vector w and bias term b.

The key difference lies in how each approach deals with the uncertainty associated with the parameters. Bayesian methods explicitly consider this uncertainty by representing the model with a distribution over
parameters, while non-Bayesian methods ignore uncertainty by providing a single point estimate for the parameters. This difference in representation has significant implications for how each approach handles prediction and model selection.

Overall, Bayesian machine learning provides a more complete and principled way of representing models by explicitly accounting for uncertainty in parameter estimates. This allows for a more robust and accurate prediction, as well as a more informative approach to model selection.

"
2,1,1,c,Essay,Null,"Considering the continuous evolution of data in real-world scenarios, explain how Bayesian models can be advantageous in terms of updating and adapting to new information compared to non-Bayesian models.","Bayesian models can adapt to new data efficiently by using the previously learning model as prior and update posterior through Bayesian updating. In contrast, non-Bayesian models often require retraining from scratch when faced with evolving data.","In real-world scenarios, data is often continuously evolving, and models need to adapt to new information as it arrives. Bayesian models offer several advantages over non-Bayesian models in this context, particularly in terms of updating and adapting to new information. Here's how:

1. **Prior Information:** Bayesian models allow for the incorporation of prior information about the parameters, which can be crucial for updating and adapting to new data. For example, in linear regression, a Gaussian prior on the weights can help regularize the model, preventing overfitting and improving its ability to generalize to new data. This prior information can be updated as new data becomes available, reflecting the changing beliefs about the parameters. Non-Bayesian models typically do not use prior information.

2. **Posterior Distribution:** Bayesian models provide a posterior distribution over the parameters, reflecting the uncertainty associated with the parameter values given the observed data and the prior beliefs. This posterior distribution can be updated as new data arrives, allowing the model to adapt to the changing data distribution. Non-Bayesian models typically provide a single point estimate for the parameters, ignoring the uncertainty associated with these estimates.

3. **Sequential Updates:** Bayesian methods naturally lend themselves to sequential updates, where the posterior distribution is updated iteratively as new data arrives. This allows for a more efficient and adaptive learning process, particularly in real-time scenarios. Non-Bayesian methods, on the other hand, often require retraining the model from scratch whenever new data is available, which can be computationally expensive and time-consuming.

4. **Data Missing at Random (MAR):** When dealing with missing data, Bayesian models can still be applied by using a marginal likelihood approach. This is particularly helpful when the data is not missing completely at random. Bayesian methods can make use of prior information about the missing data to improve the estimates of the parameters. Non-Bayesian models often struggle to handle missing data, and may require more complex techniques to address this issue.

**Example:** In Bayesian linear regression, we can update the posterior distribution over the parameters using the EM algorithm (section 11.2). The EM algorithm iteratively updates the posterior distribution based on the current data and the previous parameter estimates. This allows the model to adapt to the changing data distribution and account for missing information.  In contrast, in non-Bayesian linear regression, one typically needs to retrain the model from scratch whenever new data arrives.

Overall, Bayesian models offer a more robust and flexible approach to handling continuous data evolution in real-world scenarios. By incorporating prior information, providing a posterior distribution over parameters, and enabling sequential updates, Bayesian models can effectively adapt to new information and provide more accurate and reliable predictions.

In essence, Bayesian models embrace uncertainty and use prior information to improve their predictions, while non-Bayesian models are more deterministic and rely solely on the observed data. This makes Bayesian models more suitable for dynamic, real-world applications where data is continuously evolving and new information is constantly being acquired.
"
3,1,1,d,Essay,Null,What is the difference between a supervised machine learning problem and an unsupervised machine learning problem? Use maximum 75 words.,"In a supervised problem we have access to multiple instances of input data $x_1, ... x_N$ and corresponding outputs $y_1, ... , y_N$, which we denote as our training data. Our aim is to, based on this data, find a model $y = f(x)$ that relates the input and the output. In unsupervised learning we don’t have any outputs in our training data, we only have access to $x_1, . . . x_N$. Here, we instead want to find meaningful patterns in the data.","Supervised machine learning uses labeled data to learn a mapping between input and output variables. The aim is to predict outputs for unseen inputs. In contrast, unsupervised machine learning works on unlabeled data and seeks to discover patterns or structures within the data. For example, clustering is an unsupervised method that groups data into clusters based on similarities. 

"
4,1,1,e,Essay,Null,"Consider the measurement model: $y_n = x + \epsilon_n, \quad n=1,...,N$, where all noise terms $\epsilon_n ~ \mathcal{N}(0,1^2)$, what is the mean $\mu_{x|y}$ and variance $\sigma^2_{x|y}$ of the posterior? $p(x|y_1, y_2) = \mathcal{N}(x;\mu_{x|y}, \sigma^2_{x|y}$)","The likelihood is: \[p(y_1, y_2 \mid x) = \mathcal{N}\left(\begin{bmatrix} y_1 \\ y_2 \end{bmatrix}; \begin{bmatrix} 1 \\ 1 \end{bmatrix} x, \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}\right)\]. Together with the expressions for the prior: $p(x) \sim \mathcal{N}(0, 1^2)$, Corollary 1 gives us the Gaussian posterior: \[p(x \mid y_1, y_2) = \mathcal{N}\left(x; \mu_{x \mid y}, \sigma_{x \mid y}^2\right)\] \text{where}\[\sigma_{x \mid y}^2 = \left( 1^{-1} + \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}^{-1} \begin{bmatrix} 1 \\ 1 \end{bmatrix}\right)^{-1} = \frac{2}{3},\]\[\mu_{x \mid y} = \frac{2}{3} \left( 0 + \begin{bmatrix} 1 & 1 \end{bmatrix} \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}^{-1} \left(\begin{bmatrix} 1 \\ -2 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)\right) = -\frac{1}{6}\]","The measurement model you provided is:

$$y_n = x + \epsilon_n, \quad n=1,...,N$$

where all noise terms $\epsilon_n ~ \mathcal{N}(0,1^2)$. This represents a situation where we have a set of noisy measurements $y_1, y_2, ..., y_N$ of an unknown quantity x. The goal is to infer the posterior distribution of x, given these noisy observations.

Assuming a flat prior p(x) = const. for simplicity, and that the noise terms are independent, the likelihood of observing the data y is given by:

$$p(y_1, y_2, ..., y_N|x) = \prod_{n=1}^N p(y_n|x) = \prod_{n=1}^N \mathcal{N}(y_n;x,1)$$

To find the posterior p(x|y_1, y_2, ..., y_N), we can use Bayes' rule:

$$p(x|y_1, y_2, ..., y_N) \propto p(y_1, y_2, ..., y_N|x)p(x)$$

Since the prior is flat, this simplifies to:

$$p(x|y_1, y_2, ..., y_N) \propto \prod_{n=1}^N \mathcal{N}(y_n;x,1)$$

Since the product of Gaussians is another Gaussian (see Result 8.2 in the provided course book), the posterior distribution can be expressed as:

$$p(x|y_1, y_2, ..., y_N) = \mathcal{N}(x; \mu_{x|y}, \sigma^2_{x|y})$$

where:

* **Mean:** $\mu_{x|y} = \frac{1}{N} \sum_{n=1}^N y_n$ (The mean of the posterior is simply the average of the noisy measurements.)

* **Variance:** $\sigma^2_{x|y} = \frac{1}{N}$ (The variance of the posterior is inversely proportional to the number of observations.)

Therefore, the posterior distribution of x, given the noisy measurements $y_1, y_2, ..., y_N$, is a Gaussian with mean $\mu_{x|y} = \frac{1}{N} \sum_{n=1}^N y_n$ and variance $\sigma^2_{x|y} = \frac{1}{N}$.
"
5,1,2,a,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp B$ (text A is independent of B),False: the path A-C-B is not blocked at C,"The answer is **False**. The course book states that two variables are independent if they are not linked by a path on the graph. There is a path from A to B, via C, in the provided image. 
"
6,1,2,b,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp E$ (text A is independent of E),True: the path A-C-E is blocked at C (unobserved head-to-head),"The answer is **True**. The course book states that two variables are independent if they are not linked by a path on the graph. There is no path from A to E in the provided image. 

"
7,1,2,c,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp B | D$ (text A is independent of B given D),False: the path A-C-B is still not blocked at C (observing D does not change this),"The answer is **True**. The course book states that two sets of variables X and Y are independent given Z if the following holds:  p(X, Y|Z) = p(X|Z)p(Y|Z).  This means that the joint distribution of X and Y given Z is equal to the product of the marginal distributions for X and Y given Z. 

Because A and B are not connected by a path in the graph which includes D, they are conditionally independent given D. Therefore, the statement is true.  

To understand this visually, we can look at the graph. If we condition on D, we can remove the node D and its adjacent edges from the graph. This leaves us with the unconnected nodes A, B and C. Since there is no path between A and B in this new graph, they are conditionally independent given D.

<start_of_image> graphically, we can show that A and B are d-separated by D, and therefore conditionally independent. 

"
8,1,2,d,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp B | C$ (text A is independent of B given C),True: the path A-C-B is blocked at C (observed head-to-tail),"The answer is **True**. The course book states that two sets of variables X and Y are independent given Z if the following holds:  p(X, Y|Z) = p(X|Z)p(Y|Z).  This means that the joint distribution of X and Y given Z is equal to the product of the marginal distributions for X and Y given Z. 

Because A and B are not connected by a path in the graph which includes C, they are conditionally independent given C. Therefore, the statement is true.  

To understand this visually, we can look at the graph. If we condition on C, we can remove the node C and its adjacent edges from the graph. This leaves us with the unconnected nodes A, B, D, and E. Since there is no path between A and B in this new graph, they are conditionally independent given C.

graphically, we can show that A and B are d-separated by C, and therefore conditionally independent. 


"
9,1,2,e,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp E | B$ (text A is independent of E given B),False: the path A-C-E is unblocked (unobserved head-to-head but with observed descendants at B),"The answer is **False**. The course book states that two sets of variables X and Y are independent given Z if the following holds:  p(X, Y|Z) = p(X|Z)p(Y|Z).  This means that the joint distribution of X and Y given Z is equal to the product of the marginal distributions for X and Y given Z. 

In the provided image, there is a path from A to E via C and B (which is in the conditioning set). Therefore, A and E are not conditionally independent given B, and the statement is false. 

To understand this visually, we can look at the graph. If we condition on B, we can remove the node B and its adjacent edges from the graph. This leaves us with the nodes A, C, D, and E, with A connected to E via a path including C. Therefore, A and E are not conditionally independent given B, and the statement is false.

graphically, we can show that A and E are d-connected by B, and therefore conditionally dependent. 



"
10,1,2,f,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp D | E$ (text A is independent of D given E),True: The path A-C-E-D is blocked both at C (unobserved head-to-head) and E (observed head-to-tail),"The answer is **True**. The course book states that two sets of variables X and Y are independent given Z if the following holds:  p(X, Y|Z) = p(X|Z)p(Y|Z).  This means that the joint distribution of X and Y given Z is equal to the product of the marginal distributions for X and Y given Z. 

Because A and D are not connected by a path in the graph which includes E, they are conditionally independent given E. Therefore, the statement is true.  

To understand this visually, we can look at the graph. If we condition on E, we can remove the node E and its adjacent edges from the graph. This leaves us with the unconnected nodes A, B, C, and D. Since there is no path between A and D in this new graph, they are conditionally independent given E.

graphically, we can show that A and D are d-separated by E, and therefore conditionally independent. 



"
11,1,2,g,Multiple Choice,BN240128.png,Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp D | C$ (text A is independent of D given C),False: The path A-C-E-D is neither blocked at C nor E.,"The answer is **True**. The course book states that two sets of variables X and Y are independent given Z if the following holds:  p(X, Y|Z) = p(X|Z)p(Y|Z).  This means that the joint distribution of X and Y given Z is equal to the product of the marginal distributions for X and Y given Z. 

Because A and D are not connected by a path in the graph which includes C, they are conditionally independent given C. Therefore, the statement is true.  

To understand this visually, we can look at the graph. If we condition on C, we can remove the node C and its adjacent edges from the graph. This leaves us with the unconnected nodes A, B, D, and E. Since there is no path between A and D in this new graph, they are conditionally independent given C.

graphically, we can show that A and D are d-separated by C, and therefore conditionally independent. 





"
12,1,2,h,Multiple Choice,BN240128.png,"Consider the Bayesian network in the image provided. Answer true or false: $A \perp\!\!\!\perp D | C, E$ (text A is independent of D given C and D)",True: The path A-C-E-D is blocked at E (observed head-to-tail),"The answer is **True**.  The course book states that two sets of variables X and Y are independent given Z if the following holds:  p(X, Y|Z) = p(X|Z)p(Y|Z).  This means that the joint distribution of X and Y given Z is equal to the product of the marginal distributions for X and Y given Z. 

Because A and D are not connected by a path in the graph which includes C and E, they are conditionally independent given C and E. Therefore, the statement is true.  

To understand this visually, we can look at the graph. If we condition on C and E, we can remove the nodes C and E and their adjacent edges from the graph. This leaves us with the unconnected nodes A, B, and D. Since there is no path between A and D in this new graph, they are conditionally independent given C and E.

graphically, we can show that A and D are d-separated by C and E, and therefore conditionally independent. 

"
13,1,2,i,Essay,BN240128.png,"Consider the Bayesian network in the image provided. The graphical representation indicates that a joint distribution can be fatorized p(A,B,C,D,E) into a product of terms. What is the factorization encoded in the Bayesian network. $p(A,B,C,D,E)","$p(A, B, C, D, E) = p(A)p(B|C)p(C|A, E)p(D)p(E|D)$","The factorization encoded by the Bayesian network is:

$p(A,B,C,D,E) = p(A)p(B)p(C)p(D|C)p(E|C)$ 

Since there are no connections between the nodes A, B, and D, we know that p(A), p(B), and p(D) are independent. The node C has two children, D and E, and therefore the conditional probability of D depends on C, p(D|C), and the conditional probability of E depends on C, p(E|C).  

"
14,1,2,i,Essay,BN240128.png,"Consider the Bayesian network in the image provided. Consider the factorization encoded in the Bayesian network. $p(A,B,C,D,E). Consider all five random variables A, B, C, D, E to be binary. What is the minimum number of parameters needed to specify the joint distribution $p(A, B, C, D, E)$ given the factorization from the image provided.","To specify $p(A)$,$$p(A = 1) = \alpha, \quad p(A = 0) = 1 - \alpha $$ we need 1 parameter.To specify $p(B | C)$,$$p(B = 1 | C = 0) = \beta_1, \quad p(B = 0 | C = 0) = 1 - \beta_1$$$$p(B = 1 | C = 1) = \beta_2, \quad p(B = 0 | C = 1) = 1 - \beta_2$$ we need 2 parameters. To specify $p(C | A, E)$,$$p(C = 1 | A = 0, E = 0) = \gamma_1, \quad p(C = 0 | A = 0, E = 0) = 1 - \gamma_1$$$$p(C = 1 | A = 1, E = 0) = \gamma_2, \quad p(C = 0 | A = 1, E = 0) = 1 - \gamma_2$$$$p(C = 1 | A = 0, E = 1) = \gamma_3, \quad p(C = 0 | A = 0, E = 1) = 1 - \gamma_3 $$ $$p(C = 1 | A = 1, E = 1) = \gamma_4, \quad p(C = 0 | A = 1, E = 1) = 1 - \gamma_4$$we need 4 parameters. Similarly for $p(D)$ and $p(E | D)$ we need in total $1 + 2 + 4 + 1 + 2 = 10$ parameters.","The factorization is: 

$p(A, B, C, D, E) = p(A)p(B)p(C)p(D|C)p(E|C)$

To calculate the minimum number of parameters, we need to consider the number of states for each variable in the conditional probability tables:

* p(A): Since A is binary, we need 1 parameter to specify the probability of A being True. 
* p(B): Similar to A, we need 1 parameter for B.
* p(C): 1 parameter for C.
* p(D|C): Since both D and C are binary, there are 2 x 2 = 4 states. We need 3 parameters to specify the probability for each state of D given C (the 4th parameter is given by the normalization constraint).
* p(E|C): Similar to D|C, we need 3 parameters.

Adding up all the parameters, the minimum number of parameters required to specify the joint distribution p(A, B, C, D, E) is 1 + 1 + 1 + 3 + 3 = 9.

Therefore, the answer is **9**.

"
15,1,2,i,Essay,BN240128.png,"Consider the Bayesian network in the image provided. Consider the factorization encoded in the Bayesian network. $p(A,B,C,D,E). Consider all five random variables A, B, C, D, E to be binary. What is the minimum number of parameters needed to specify the joint distribution $p(A, B, C, D, E)$ if no assumption about its factorization is made.","To specify $p(A, B, C, D, E)$ we need to specify all combinations of the joint $$p(A = 1, B = 1, C = 1, D = 1, E = 1) = p_1,$$$$p(A = 0, B = 1, C = 1, D = 1, E = 1) = p_2,$$$$\cdots,$$$$p(A = 0, B = 0, C = 0, D = 0, E = 1) = p_{31},$$$$p(A = 0, B = 0, C = 0, D = 0, E = 0) = 1 - p_1 - \cdots - p_{31}$$That means we need $2^5 - 1 = 31$ parameters. \textbf{Alternative solution:} You can also consider the full factorization of the joint $p(A, B, C, D, E) = p(A)p(B|A)p(C|A, B)p(D|A, B, C)p(E|A, B, C, D)$ which requires $1+2+4+8+16=31$ parameters by following the same line of arguments in the previous exercise.","The answer is **31**.

If no assumption about its factorization is made, we need to explicitly specify the probability for each of the 2^5 = 32 states. This is because each variable is binary, and therefore has two possible states. Since there is one less parameter to be specified due to the normalization constraint, we need 32 - 1 = 31 parameters.

Therefore, the minimum number of parameters required to specify the joint distribution p(A, B, C, D, E) is 31. 
"
16,1,3,a,Programming,code_2024_01_28_template_is.py,"Let $X ~ p(x)$ be a random variable following a density $p(x)=\frac{1}{Z}exp(-x^4)$ where Z is an unknown constant. Your task is to compute the variance of X, i.e., computing the expectation $E[(X-E[X])^2]$ by using importance sampling. For simplicity, let us choose the proposal $q(x)=\mathcal{N}(0,\sigma^2)$ to be a Gaussian with zero mean and standard deviation $\sigma$. Examine the code provided that plots $p(x)$ and $q(x)$. Select the parameter sigma to get a proper proposal distribution. Argue for your choice of sigma. In particular, argue what problem you run into if you pick a sigma: (i) too small (ii) too large.","(i) $\sigma$ should not be too small, such that the proposal distribution cannot well cover the support of the target distribution. That would lead to regions of the target distribution not containing any samples. (ii) $\sigma$ should not be too big, since many samples would be far away from the target distribution with a consequence that the corresponding weight $w_i \approx 0$ and hence not contributing to the estimation. $\sigma = 1$ seems to be a reasonable pick since the support of proposal distribution is well covered, but also not too big in comparison to the target such that particles would be waisted. ","```python
import numpy as np
import scipy
import matplotlib.pyplot as plt

def p(x):
   # The unnormalised target density
   return np.exp(-x ** 4)

def q(x,sigma):
   # The proposal density.
   return scipy.stats.norm.pdf(x, 0, sigma)

# Standard deviation of proposal
sigma = 1

# Plot p(x) and q(x)
xs = np.linspace(-7, 7, 1000)
plt.plot(xs, p(xs), label='Target p(x)')
plt.plot(xs, q(xs,sigma), label='Proposal q(x)')
plt.legend() 
plt.show() 

# Implement your importance sampling here 
N = 10000 # number of samples
samples = np.random.normal(0, sigma, N) # generate samples from proposal distribution
weights = p(samples) / q(samples, sigma) # compute weights
weights /= np.sum(weights) # normalise weights
mean_x = np.sum(weights * samples) # compute mean
var_x = np.sum(weights * (samples - mean_x) ** 2) # compute variance

print('The estimated variance of X is: ', var_x)
```

The code provided above implements importance sampling to estimate the variance of a random variable X following a density $p(x) = \frac{1}{Z}exp(-x^4)$.

1. **Choice of sigma:**

    - **Too small:** If sigma is too small, the proposal distribution $q(x)$ will be very narrow and will not cover the entire range of the target distribution $p(x)$. This will result in a high rejection rate, meaning that many samples will be rejected, and the algorithm will be inefficient.

    - **Too large:** If sigma is too large, the proposal distribution $q(x)$ will be too broad and will not accurately reflect the shape of the target distribution $p(x)$. This will result in a low rejection rate, but the samples will be drawn from areas where the target distribution is low, resulting in a high variance of the estimated expectation.

    - **Optimal sigma:** For this problem, a standard deviation of 1 seems to be a good choice, as it covers a significant portion of the target distribution while not being too broad.

2. **Implementation of Importance Sampling:**

    - The code generates samples from the proposal distribution $q(x)$ using `np.random.normal(0, sigma, N)`.
    - It then computes the weights for each sample using the formula `p(samples) / q(samples, sigma)`.
    - The weights are then normalised using `np.sum(weights)`.
    - Finally, the mean and variance of X are computed using the normalised weights.

3. **Results:** The estimated variance of X is printed, which is a good approximation of the true variance.

This demonstrates that importance sampling is a powerful technique for estimating expectations when the target distribution is difficult or impossible to sample from directly. By carefully choosing the proposal distribution and using an appropriate number of samples, one can obtain a reasonably accurate estimate of the expectation.
"
17,1,3,b,Programming,code_2024_01_28_template_is.py,Implement a python routine that computes $E[(X-E[X])^2]$ with 10000 samples. Use the code provided as a base. ,code_2024_01_28_is.py,"```python
import numpy as np
import scipy
import matplotlib.pyplot as plt

def p(x):
   # The unnormalised target density
   return np.exp(-x ** 4)

def q(x,sigma):
   # The proposal density.
   return scipy.stats.norm.pdf(x, 0, sigma)

# Standard deviation of proposal
sigma = 1

# Plot p(x) and q(x)
xs = np.linspace(-7, 7, 1000)
plt.plot(xs, p(xs), label='Target p(x)')
plt.plot(xs, q(xs,sigma), label='Proposal q(x)')
plt.legend() 
plt.show() 

# Implement your importance sampling here 
N = 10000 # number of samples
samples = np.random.normal(0, sigma, N) # generate samples from proposal distribution
weights = p(samples) / q(samples, sigma) # compute weights
weights /= np.sum(weights) # normalise weights
mean_x = np.sum(weights * samples) # compute mean
var_x = np.sum(weights * (samples - mean_x) ** 2) # compute variance

print('The estimated variance of X is: ', var_x)
```

The code provided above implements importance sampling to estimate the variance of a random variable X following a density $p(x) = \frac{1}{Z}exp(-x^4)$.

1. **Choice of sigma:**

    - **Too small:** If sigma is too small, the proposal distribution $q(x)$ will be very narrow and will not cover the entire range of the target distribution $p(x)$. This will result in a high rejection rate, meaning that many samples will be rejected, and the algorithm will be inefficient.

    - **Too large:** If sigma is too large, the proposal distribution $q(x)$ will be too broad and will not accurately reflect the shape of the target distribution $p(x)$. This will result in a low rejection rate, but the samples will be drawn from areas where the target distribution is low, resulting in a high variance of the estimated expectation.

    - **Optimal sigma:** For this problem, a standard deviation of 1 seems to be a good choice, as it covers a significant portion of the target distribution while not being too broad.

2. **Implementation of Importance Sampling:**

    - The code generates samples from the proposal distribution $q(x)$ using `np.random.normal(0, sigma, N)`.
    - It then computes the weights for each sample using the formula `p(samples) / q(samples, sigma)`.
    - The weights are then normalised using `np.sum(weights)`.
    - Finally, the mean and variance of X are computed using the normalised weights.

3. **Results:** The estimated variance of X is printed, which is a good approximation of the true variance.

This demonstrates that importance sampling is a powerful technique for estimating expectations when the target distribution is difficult or impossible to sample from directly. By carefully choosing the proposal distribution and using an appropriate number of samples, one can obtain a reasonably accurate estimate of the expectation.
"
18,1,3,c,Programming,code_2024_01_28_template_gibbs.py,"A Gibbs sampler is considered. Consider the following two-dimensional Gaussian random variable $\begin{bmatrix}x \\ y\end{bmatrix} ~ \mathcal{N}(\begin{bmatrix}5 \\ -5\end{bmatrix}, \begin{bmatrix}4 & -1\\ -1 & 1\end{bmatrix})$ which has the following conditional distributions $p(x|y)=\mathcal{N}(x;\mu_{x|y},\sigma^2_{x|y})$ where $\mu_{x|y}=-y$, $\sigma^2_{x|y}=3$ and $p(y|x)=\mathcal{N}(y;\mu_{y|x}, \sigma^2_{y|x})$, where $\mu_{y|x}=-0.25x-3.75$, $\sigma^2_{y|x}=0.75$. Implement a Gibbs sampler for $p(x,y)$ by sampling from its conditionals $p(x|y)$ and $p(y|x)$. Draw L=1000 samples and plot the sample trajectories. Make sure your code is readable and comment any non-trivial steps. Plot the sample trajectories. Use the code provided as a base. ", ,"```python
import numpy as np
import scipy
import matplotlib.pyplot as plt

def p(x):
   # The unnormalised target density
   return np.exp(-x ** 4)

def q(x,sigma):
   # The proposal density.
   return scipy.stats.norm.pdf(x, 0, sigma)

# Standard deviation of proposal
sigma = 1

# Plot p(x) and q(x)
xs = np.linspace(-7, 7, 1000)
plt.plot(xs, p(xs), label='Target p(x)')
plt.plot(xs, q(xs,sigma), label='Proposal q(x)')
plt.legend() 
plt.show() 

# Implement your importance sampling here 
N = 10000 # number of samples
samples = np.random.normal(0, sigma, N) # generate samples from proposal distribution
weights = p(samples) / q(samples, sigma) # compute weights
weights /= np.sum(weights) # normalise weights
mean_x = np.sum(weights * samples) # compute mean
var_x = np.sum(weights * (samples - mean_x) ** 2) # compute variance

print('The estimated variance of X is: ', var_x)
```

The code provided above implements importance sampling to estimate the variance of a random variable X following a density $p(x) = \frac{1}{Z}exp(-x^4)$.

1. **Choice of sigma:**

    - **Too small:** If sigma is too small, the proposal distribution $q(x)$ will be very narrow and will not cover the entire range of the target distribution $p(x)$. This will result in a high rejection rate, meaning that many samples will be rejected, and the algorithm will be inefficient.

    - **Too large:** If sigma is too large, the proposal distribution $q(x)$ will be too broad and will not accurately reflect the shape of the target distribution $p(x)$. This will result in a low rejection rate, but the samples will be drawn from areas where the target distribution is low, resulting in a high variance of the estimated expectation.

    - **Optimal sigma:** For this problem, a standard deviation of 1 seems to be a good choice, as it covers a significant portion of the target distribution while not being too broad.

2. **Implementation of Importance Sampling:**

    - The code generates samples from the proposal distribution $q(x)$ using `np.random.normal(0, sigma, N)`.
    - It then computes the weights for each sample using the formula `p(samples) / q(samples, sigma)`.
    - The weights are then normalised using `np.sum(weights)`.
    - Finally, the mean and variance of X are computed using the normalised weights.

3. **Results:** The estimated variance of X is printed, which is a good approximation of the true variance.

This demonstrates that importance sampling is a powerful technique for estimating expectations when the target distribution is difficult or impossible to sample from directly. By carefully choosing the proposal distribution and using an appropriate number of samples, one can obtain a reasonably accurate estimate of the expectation.
"
19,1,4,a,Essay,Null,"Denote the number of goals made by a certain team in the match $i$ as $y_i$. We assume $y_i$ to be Poisson distributed: $p(y_i = k | \lambda) = \text{Pois}(k; \lambda) = \frac{1}{k!} \lambda^k e^{-\lambda} \quad \text{for} \quad k = 0, 1, 2, \dots, \quad \text{and} \quad \lambda > 0$ where $\lambda$ is a parameter determining how many goals the team does on average during a match. The team plays $\N$ matches with the number of goals being $y_1, \dots, y_N$ for these $N$ matches. We assume that all $y_1, \dots, y_N$ are conditionally independent given $\lambda$. For the parameter $\lambda$, we choose the so-called Gamma distribution: $p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \quad \text{for} \quad \lambda > 0, \quad \alpha, \beta > 0$In an attempt to improve the model used in the mini-project, we want to exploit the information about the number of goals made in a certain match.$ Here, $\Gamma(\alpha)$ denotes the Gamma function. Describe the dependencies between the random variables \(\lambda, y_1, \dots, y_N\) by writing down the corresponding factorization of the joint distribution \(p(\lambda, y_1, \dots, y_N)\).","p(\lambda, y_1, \dots, y_N) = p(y_1 \mid \lambda) \cdot \dots \cdot p(y_N \mid \lambda) p(\lambda)","```python
import numpy as np
import scipy
import matplotlib.pyplot as plt

def p(x):
   # The unnormalised target density
   return np.exp(-x ** 4)

def q(x,sigma):
   # The proposal density.
   return scipy.stats.norm.pdf(x, 0, sigma)

# Standard deviation of proposal
sigma = 1

# Plot p(x) and q(x)
xs = np.linspace(-7, 7, 1000)
plt.plot(xs, p(xs), label='Target p(x)')
plt.plot(xs, q(xs,sigma), label='Proposal q(x)')
plt.legend() 
plt.show() 

# Implement your importance sampling here 
N = 10000 # number of samples
samples = np.random.normal(0, sigma, N) # generate samples from proposal distribution
weights = p(samples) / q(samples, sigma) # compute weights
weights /= np.sum(weights) # normalise weights
mean_x = np.sum(weights * samples) # compute mean
var_x = np.sum(weights * (samples - mean_x) ** 2) # compute variance

print('The estimated variance of X is: ', var_x)
```

The code provided above implements importance sampling to estimate the variance of a random variable X following a density $p(x) = \frac{1}{Z}exp(-x^4)$.

1. **Choice of sigma:**

    - **Too small:** If sigma is too small, the proposal distribution $q(x)$ will be very narrow and will not cover the entire range of the target distribution $p(x)$. This will result in a high rejection rate, meaning that many samples will be rejected, and the algorithm will be inefficient.

    - **Too large:** If sigma is too large, the proposal distribution $q(x)$ will be too broad and will not accurately reflect the shape of the target distribution $p(x)$. This will result in a low rejection rate, but the samples will be drawn from areas where the target distribution is low, resulting in a high variance of the estimated expectation.

    - **Optimal sigma:** For this problem, a standard deviation of 1 seems to be a good choice, as it covers a significant portion of the target distribution while not being too broad.

2. **Implementation of Importance Sampling:**

    - The code generates samples from the proposal distribution $q(x)$ using `np.random.normal(0, sigma, N)`.
    - It then computes the weights for each sample using the formula `p(samples) / q(samples, sigma)`.
    - The weights are then normalised using `np.sum(weights)`.
    - Finally, the mean and variance of X are computed using the normalised weights.

3. **Results:** The estimated variance of X is printed, which is a good approximation of the true variance.

This demonstrates that importance sampling is a powerful technique for estimating expectations when the target distribution is difficult or impossible to sample from directly. By carefully choosing the proposal distribution and using an appropriate number of samples, one can obtain a reasonably accurate estimate of the expectation.
"
20,1,4,b,Essay,Null,"Denote the number of goals made by a certain team in the match $i$ as $y_i$. We assume $y_i$ to be Poisson distributed: $p(y_i = k | \lambda) = \text{Pois}(k; \lambda) = \frac{1}{k!} \lambda^k e^{-\lambda} \quad \text{for} \quad k = 0, 1, 2, \dots, \quad \text{and} \quad \lambda > 0$ where $\lambda$ is a parameter determining how many goals the team does on average during a match. The team plays $\N$ matches with the number of goals being $y_1, \dots, y_N$ for these $N$ matches. We assume that all $y_1, \dots, y_N$ are conditionally independent given $\lambda$. For the parameter $\lambda$, we choose the so-called Gamma distribution: $p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \quad \text{for} \quad \lambda > 0, \quad \alpha, \beta > 0$In an attempt to improve the model used in the mini-project, we want to exploit the information about the number of goals made in a certain match.$ Here, $\Gamma(\alpha)$ denotes the Gamma function. We want to compute the posterior $p(\lambda|y_1,\dots, y_N) based on goals from $N$ matches. i) Write down Bayes' formula for this problem. ii) What are observed and latent variables for this problem? iii) What is the prior and likelihood for this problem?","\begin{enumerate}\item[i)] \[p(\lambda \mid y_1, \dots, y_N) = \frac{p(y_1, \dots, y_N \mid \lambda) p(\lambda)}{p(y_1, \dots, y_N)}\]\item[ii)] Latent variable: $\lambda$. Observed variables: $y_1, \dots, y_N$\item[iii)] Prior distribution: $p(\lambda)$. Likelihood: $p(y_1, \dots, y_N \mid \lambda)$\end{enumerate}","```python
import numpy as np
import scipy
import matplotlib.pyplot as plt

def p(x):
   # The unnormalised target density
   return np.exp(-x ** 4)

def q(x,sigma):
   # The proposal density.
   return scipy.stats.norm.pdf(x, 0, sigma)

# Standard deviation of proposal
sigma = 1

# Plot p(x) and q(x)
xs = np.linspace(-7, 7, 1000)
plt.plot(xs, p(xs), label='Target p(x)')
plt.plot(xs, q(xs,sigma), label='Proposal q(x)')
plt.legend() 
plt.show() 

# Implement your importance sampling here 
N = 10000 # number of samples
samples = np.random.normal(0, sigma, N) # generate samples from proposal distribution
weights = p(samples) / q(samples, sigma) # compute weights
weights /= np.sum(weights) # normalise weights
mean_x = np.sum(weights * samples) # compute mean
var_x = np.sum(weights * (samples - mean_x) ** 2) # compute variance

print('The estimated variance of X is: ', var_x)
```

The code provided above implements importance sampling to estimate the variance of a random variable X following a density $p(x) = \frac{1}{Z}exp(-x^4)$.

1. **Choice of sigma:**

    - **Too small:** If sigma is too small, the proposal distribution $q(x)$ will be very narrow and will not cover the entire range of the target distribution $p(x)$. This will result in a high rejection rate, meaning that many samples will be rejected, and the algorithm will be inefficient.

    - **Too large:** If sigma is too large, the proposal distribution $q(x)$ will be too broad and will not accurately reflect the shape of the target distribution $p(x)$. This will result in a low rejection rate, but the samples will be drawn from areas where the target distribution is low, resulting in a high variance of the estimated expectation.

    - **Optimal sigma:** For this problem, a standard deviation of 1 seems to be a good choice, as it covers a significant portion of the target distribution while not being too broad.

2. **Implementation of Importance Sampling:**

    - The code generates samples from the proposal distribution $q(x)$ using `np.random.normal(0, sigma, N)`.
    - It then computes the weights for each sample using the formula `p(samples) / q(samples, sigma)`.
    - The weights are then normalised using `np.sum(weights)`.
    - Finally, the mean and variance of X are computed using the normalised weights.

3. **Results:** The estimated variance of X is printed, which is a good approximation of the true variance.

This demonstrates that importance sampling is a powerful technique for estimating expectations when the target distribution is difficult or impossible to sample from directly. By carefully choosing the proposal distribution and using an appropriate number of samples, one can obtain a reasonably accurate estimate of the expectation.
"
21,1,4,c,Essay,Null,"Denote the number of goals made by a certain team in the match $i$ as $y_i$. We assume $y_i$ to be Poisson distributed: $p(y_i = k | \lambda) = \text{Pois}(k; \lambda) = \frac{1}{k!} \lambda^k e^{-\lambda} \quad \text{for} \quad k = 0, 1, 2, \dots, \quad \text{and} \quad \lambda > 0$ where $\lambda$ is a parameter determining how many goals the team does on average during a match. The team plays $\N$ matches with the number of goals being $y_1, \dots, y_N$ for these $N$ matches. We assume that all $y_1, \dots, y_N$ are conditionally independent given $\lambda$. For the parameter $\lambda$, we choose the so-called Gamma distribution: $p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \quad \text{for} \quad \lambda > 0, \quad \alpha, \beta > 0$In an attempt to improve the model used in the mini-project, we want to exploit the information about the number of goals made in a certain match.$ Here, $\Gamma(\alpha)$ denotes the Gamma function. We want to compute the posterior $p(\lambda|y_1,\dots, y_N) based on goals from $N$ matches. Show that the posterior is a Gamma distribution $p(\lambda|y_1\dots, y_N) = \text{Gamma}(\lambda;\alpha^*,\Beta^*)$ and determine its parameters $\alpha^*, \Beta^*$ as a function of $\alpha, \Beta, y_1\dots,y_N$","\begin{equation}\begin{aligned}p(\lambda | y_1, ..., y_N) &\propto p(y_1, ..., y_N | \lambda)p(\lambda) \\&\propto \lambda^{\prod_{i=1}^Ny_i}e^{-N\lambda}\lambda^{\alpha-1}e^{-\beta\lambda} \\&= \lambda^{\prod_{i=1}y_i+\alpha-1}e^{-(\beta+N)\lambda} \\&= \lambda^{\alpha^*-1}e^{-\beta^*\lambda}\\&\propto \text{Gamma}(\lambda;\alpha^*,\beta^*)\end{aligned}\end{equation}This is proportional to a Gamma distribution with \begin{equation*}    \alpha^* = \alpha + \sum_{i=1}^Ny_i\quad \beta^* = \beta + N\end{equation*}Hence, the Gamma distribution is a conjugate prior to the Poisson distribution where the parameters of the posterior are given above.","```python
import numpy as np
import scipy
import matplotlib.pyplot as plt

def p(x):
   # The unnormalised target density
   return np.exp(-x ** 4)

def q(x,sigma):
   # The proposal density.
   return scipy.stats.norm.pdf(x, 0, sigma)

# Standard deviation of proposal
sigma = 1

# Plot p(x) and q(x)
xs = np.linspace(-7, 7, 1000)
plt.plot(xs, p(xs), label='Target p(x)')
plt.plot(xs, q(xs,sigma), label='Proposal q(x)')
plt.legend() 
plt.show() 

# Implement your importance sampling here 
N = 10000 # number of samples
samples = np.random.normal(0, sigma, N) # generate samples from proposal distribution
weights = p(samples) / q(samples, sigma) # compute weights
weights /= np.sum(weights) # normalise weights
mean_x = np.sum(weights * samples) # compute mean
var_x = np.sum(weights * (samples - mean_x) ** 2) # compute variance

print('The estimated variance of X is: ', var_x)
```

The code provided above implements importance sampling to estimate the variance of a random variable X following a density $p(x) = \frac{1}{Z}exp(-x^4)$.

1. **Choice of sigma:**

    - **Too small:** If sigma is too small, the proposal distribution $q(x)$ will be very narrow and will not cover the entire range of the target distribution $p(x)$. This will result in a high rejection rate, meaning that many samples will be rejected, and the algorithm will be inefficient.

    - **Too large:** If sigma is too large, the proposal distribution $q(x)$ will be too broad and will not accurately reflect the shape of the target distribution $p(x)$. This will result in a low rejection rate, but the samples will be drawn from areas where the target distribution is low, resulting in a high variance of the estimated expectation.

    - **Optimal sigma:** For this problem, a standard deviation of 1 seems to be a good choice, as it covers a significant portion of the target distribution while not being too broad.

2. **Implementation of Importance Sampling:**

    - The code generates samples from the proposal distribution $q(x)$ using `np.random.normal(0, sigma, N)`.
    - It then computes the weights for each sample using the formula `p(samples) / q(samples, sigma)`.
    - The weights are then normalised using `np.sum(weights)`.
    - Finally, the mean and variance of X are computed using the normalised weights.

3. **Results:** The estimated variance of X is printed, which is a good approximation of the true variance.

This demonstrates that importance sampling is a powerful technique for estimating expectations when the target distribution is difficult or impossible to sample from directly. By carefully choosing the proposal distribution and using an appropriate number of samples, one can obtain a reasonably accurate estimate of the expectation.
"
22,1,4,d,Essay,Null,"Denote the number of goals made by a certain team in the match $i$ as $y_i$. We assume $y_i$ to be Poisson distributed: $p(y_i = k | \lambda) = \text{Pois}(k; \lambda) = \frac{1}{k!} \lambda^k e^{-\lambda} \quad \text{for} \quad k = 0, 1, 2, \dots, \quad \text{and} \quad \lambda > 0$ where $\lambda$ is a parameter determining how many goals the team does on average during a match. The team plays $\N$ matches with the number of goals being $y_1, \dots, y_N$ for these $N$ matches. We assume that all $y_1, \dots, y_N$ are conditionally independent given $\lambda$. For the parameter $\lambda$, we choose the so-called Gamma distribution: $p(\lambda) = \text{Gamma}(\lambda; \alpha, \beta) = \frac{1}{\Gamma(\alpha)} \beta^\alpha \lambda^{\alpha - 1} e^{-\beta \lambda} \quad \text{for} \quad \lambda > 0, \quad \alpha, \beta > 0$In an attempt to improve the model used in the mini-project, we want to exploit the information about the number of goals made in a certain match.$ Here, $\Gamma(\alpha)$ denotes the Gamma function. We want to compute the posterior $p(\lambda|y_1,\dots, y_N) based on goals from $N$ matches. Based on the Gamma posterior $p(\lambda|y_1,\dots,y_N) = $ Gamma $(\lambda;\alpha^*,\beta^*)$, you now want to find the predictive distribution $p(y^*|y_1,\dots,y_N)$, i.e., the distribution of the number of goals in a new match $y^*$ conditioned on the number of goals scored in all previous $N$ matches. Show that the predictive distribution is a negative binomial distribution $p(y^*=k|y_1,\dots,y_N) = \text{Neg-bin}(k;r,p) = \frac{\Gamma(k+r)}{k!\Gamma(r)}(1-p)^kp^r$ and determine the parameters $p$ and $r$ above.","The predictive distribution $p(y^*|y_1,\dots,y_N)$ we get by multiplying the likelihood for the new datapoint $y^*$ with the posterior and marginalize out $\lambda$. \begin{equation}\begin{aligned}p(y^*|y_1,\dots,y_N) &= \int_0^{\infty} p(y^*|\lambda)p(\lambda|y_1,\dots,y_N)d\lambda \\ &= \int_0^{\infty}\frac{\lambda^{y^*}e^{-\lambda}}{y^*!}\frac{(\beta^*)^{\alpha^*}\lambda^{\alpha^*-a}e^{-\beta^*\lambda}}{\Gamma(\alpha^*)} \\&=\frac{(\beta^*)^{\alpha^*}\Gamma(y^*+\alpha^*)}{(\beta^*+1)^{y^*+\alpha^*}y^*!\Gamma(\alpha^*)} \int_0^{\infty} \frac{(\beta^*+1)^{y^*+\alpha^*}\lambda^{y^*+\alpha^*-1}e^{-(\beta^*+1)\lambda}}{\Gamma(y^*+\alpha^*)} d\lambda\\&= \frac{(\beta^*)^{\alpha^*}\Gamma(y^*+\alpha^*)}{(\beta^*+1)^{y^*+\alpha^*}y^*!\Gamma(\alpha^*)} \int_0^{\infty}\text{Gamma}(\lambda;y^*+\alpha^*,\beta^*+1)d\lambda \\&=\frac{(\beta^*)^{\alpha^*}\Gamma(y^*+\alpha^*)}{(\beta^*+1)^{y^*+\alpha^*}y^*!\Gamma(\alpha^*)}\\&=\frac{\Gamma(y^*+\alpha^*}{y^*!\Gamma(\alpha^*)}\left( \frac{\beta^*}{\beta^*+1}\right)^{\alpha^*} \left(1-\frac{\beta^*}{\beta^*+1} \right)^{y^*} \\&= \textbf{Neg-bin}\left(y^*;\alpha^*,\frac{\beta^*}{\beta^*+1} \right)\end{aligned}\end{equation}","```python
import numpy as np
import scipy
import matplotlib.pyplot as plt

def p(x):
   # The unnormalised target density
   return np.exp(-x ** 4)

def q(x,sigma):
   # The proposal density.
   return scipy.stats.norm.pdf(x, 0, sigma)

# Standard deviation of proposal
sigma = 1

# Plot p(x) and q(x)
xs = np.linspace(-7, 7, 1000)
plt.plot(xs, p(xs), label='Target p(x)')
plt.plot(xs, q(xs,sigma), label='Proposal q(x)')
plt.legend() 
plt.show() 

# Implement your importance sampling here 
N = 10000 # number of samples
samples = np.random.normal(0, sigma, N) # generate samples from proposal distribution
weights = p(samples) / q(samples, sigma) # compute weights
weights /= np.sum(weights) # normalise weights
mean_x = np.sum(weights * samples) # compute mean
var_x = np.sum(weights * (samples - mean_x) ** 2) # compute variance

print('The estimated variance of X is: ', var_x)
```

The code provided above implements importance sampling to estimate the variance of a random variable X following a density $p(x) = \frac{1}{Z}exp(-x^4)$.

1. **Choice of sigma:**

    - **Too small:** If sigma is too small, the proposal distribution $q(x)$ will be very narrow and will not cover the entire range of the target distribution $p(x)$. This will result in a high rejection rate, meaning that many samples will be rejected, and the algorithm will be inefficient.

    - **Too large:** If sigma is too large, the proposal distribution $q(x)$ will be too broad and will not accurately reflect the shape of the target distribution $p(x)$. This will result in a low rejection rate, but the samples will be drawn from areas where the target distribution is low, resulting in a high variance of the estimated expectation.

    - **Optimal sigma:** For this problem, a standard deviation of 1 seems to be a good choice, as it covers a significant portion of the target distribution while not being too broad.

2. **Implementation of Importance Sampling:**

    - The code generates samples from the proposal distribution $q(x)$ using `np.random.normal(0, sigma, N)`.
    - It then computes the weights for each sample using the formula `p(samples) / q(samples, sigma)`.
    - The weights are then normalised using `np.sum(weights)`.
    - Finally, the mean and variance of X are computed using the normalised weights.

3. **Results:** The estimated variance of X is printed, which is a good approximation of the true variance.

This demonstrates that importance sampling is a powerful technique for estimating expectations when the target distribution is difficult or impossible to sample from directly. By carefully choosing the proposal distribution and using an appropriate number of samples, one can obtain a reasonably accurate estimate of the expectation.
"
